---
title: "Analysis"
author: "Zach"
date: "2024-02-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(mgcv)
library(ggpubr)
library(tidybayes)

corpus = read_csv('../Data/corpus.csv')
```

# Analysis

Within this script will be the analyses for the study about ordering preferences in LLMs.

We care about how the effects of generative preferences and observed (relative) frequency change as a function of the overall frequency of the binomial.

This script will be divided into two main sections:

1.  The analyses for just the binomial (*cat and dog*)
2.  The analyses for the binomial within a sentence context*.*

We will run analyses for reading times and 2afc task. 2afc task is operationalized here as the sum of the log probabilities across all words in the sentence. Reading time is operationalized as the sum of the log probabilities across our target region, consists of 6 words: the first word in the binomial to the 6th word after.

## Just Binomials

### 2afc

Technically, for just binomials, 2afc = reading times. Since 2afc is the probability of the sentence (sum of log probs for each item in the sentence), and reading times are the probability of a specific region in the sentence (sum of log probs for each item in a given region of the sentence).

In this case, the entire sentence is the region we care about. Thus for just binomials we run only one task for each language model.

Our main analyses will be:

$$
logit(P_{AandB}) \sim GenPref + OrderingPref + Overall Freq + OverallFreq \colon GenPref + OverallFreq \colon OrderingPref
$$

Where our dependent variable is the probability of the binomial being alphabetical form.

And the independent variables are the generative preference of a given binomial, the ordering preference, the overall frequency, and the interaction between overall frequency and generative preference as well as the interaction between overall frequency and the ordering preference.

We don't need a random intercept for item because we have one observation for each item, so this would be meaningless.

We also don't need a random intercept for "subject" because we don't have any subjects.

<!--# do we have no random effects here?? Surely I'm forgetting something -->

#### GPT2

Load data:

```{r}
gpt2_data_just_binoms = read_csv('../Data/gpt2_2afc_binom_ordering_prefs.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)
```

Quick and dirty historgram:

```{r}
# ggplot(data = gpt2_data_just_binoms, aes(x=ProbAandB, y = ..density..)) +
#   geom_histogram() +
#   theme_bw()
# 
# ggplot(data = gpt2xl_data_just_binoms, aes(x=ProbAandB, y = ..density..)) +
#   geom_histogram() +
#   theme_bw()
# 
# ggplot(data = llama7b_data_just_binoms, aes(x=ProbAandB, y = ..density..)) +
#   geom_histogram() +
#   theme_bw()
# 
# ggplot(data = llama13b_data_just_binoms, aes(x=ProbAandB, y = ..density..)) +
#   geom_histogram() +
#   theme_bw()
# 
# ggplot(data = corpus, aes(x=RelFreq, y = ..density..)) +
#   geom_histogram() +
#   theme_bw()
```

Analysis:

```{r}
prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

gpt2_2afc_model_just_binoms = brm(data = gpt2_data_just_binoms,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 20000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/gpt2_2afc_model_just_binoms'
                      )

fixef(gpt2_2afc_model_just_binoms)

# mcmc_plot(
#   gpt2_2afc_model_just_binoms,
#   type = 'pairs',
#   off_diag_fun = 'hex',
#   diag_fun = 'dens',
#   fixed = T
# )
```

Checking posterior samples:

```{r}
post_samples_gpt2 = as.data.frame(fixef(gpt2_2afc_model_just_binoms, summary = F))

post_samples_genpref = sum(post_samples_gpt2$GenPref < 0) / length(post_samples_gpt2$GenPref)
  
post_samples_genpref_freq = sum(post_samples_gpt2$`GenPref:OverallFreq` < 0) / length(post_samples_gpt2$`GenPref:OverallFreq`)

print(post_samples_genpref)
print(post_samples_genpref_freq)
```

#### GPT2-XL

Load data:

```{r}
gpt2xl_data_just_binoms = read_csv('../Data/gpt2xl_2afc_binom_ordering_prefs.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)


```

Analysis:

```{r}
options(contrasts = c("contr.sum","contr.sum"))

gpt2xl_2afc_model_just_binoms = brm(data = gpt2xl_data_just_binoms,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 20000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_2afc_model_just_binoms'
                      )

fixef(gpt2xl_2afc_model_just_binoms)
```

Checking posterior samples for some of the effects:

```{r}
post_samples_gpt2xl = as.data.frame(fixef(gpt2xl_2afc_model_just_binoms, summary = F))

post_samples_genpref = sum(post_samples_gpt2xl$GenPref < 0) / length(post_samples_gpt2xl$GenPref)
  
post_samples_genpref_freq = sum(post_samples_gpt2xl$`GenPref:OverallFreq` < 0) / length(post_samples_gpt2xl$`GenPref:OverallFreq`)

# post_samples_relfreq_freq = sum(post_samples_gpt2$`RelFreq:OverallFreq` > 0) / length(post_samples_gpt2$`RelFreq:OverallFreq`)

print(post_samples_genpref)
print(post_samples_genpref_freq)

```

#### Llama-7b

Load data:

```{r}
llama7b_data_just_binoms = read_csv('../Data/llama7b_2afc_binom_ordering_prefs.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)
```

Analysis:

```{r}
options(contrasts = c("contr.sum","contr.sum"))

llama7b_2afc_model_just_binoms = brm(data = llama7b_data_just_binoms,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 20000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/llama7b_2afc_model_just_binoms'
                      )

fixef(llama7b_2afc_model_just_binoms)
```

Checking posterior:

```{r}
post_samples_llama7b = as.data.frame(fixef(llama7b_2afc_model_just_binoms, summary = F))

post_samples_genpref = sum(post_samples_llama7b$GenPref < 0) / length(post_samples_llama7b$GenPref)
  
post_samples_genpref_freq = sum(post_samples_llama7b$`GenPref:OverallFreq` < 0) / length(post_samples_llama7b$`GenPref:OverallFreq`)

post_samples_relfreq_freq = sum(post_samples_llama7b$`RelFreq:OverallFreq` < 0) / length(post_samples_llama7b$`RelFreq:OverallFreq`)

print(post_samples_genpref)
print(post_samples_genpref_freq)
print(post_samples_relfreq_freq)
```

#### Llama-13b

Load data:

```{r}
llama13b_data_just_binoms = read_csv('../Data/llama13b_2afc_binom_ordering_prefs.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) 

llama13b_data_just_binoms = llama13b_data_just_binoms %>%
  mutate(WordA = tolower(WordA), WordB = tolower(WordB))

llama13b_data_just_binoms = llama13b_data_just_binoms %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)
```

Analysis:

```{r}
options(contrasts = c("contr.sum","contr.sum"))
llama13b_2afc_model_just_binoms = brm(data = llama13b_data_just_binoms,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 20000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/llama13b_2afc_model_just_binoms'
                      )

fixef(llama13b_2afc_model_just_binoms)
```

Checking posterior:

```{r}
post_samples_llama13b = as.data.frame(fixef(llama13b_2afc_model_just_binoms, summary = F))

# post_samples_genpref = sum(post_samples_llama13b$GenPref < 0) / length(post_samples_llama13b$GenPref)
  
post_samples_genpref_freq = sum(post_samples_llama13b$`GenPref:OverallFreq` < 0) / length(post_samples_llama13b$`GenPref:OverallFreq`)

post_samples_relfreq_freq = sum(post_samples_llama13b$`RelFreq:OverallFreq` < 0) / length(post_samples_llama13b$`RelFreq:OverallFreq`)

print(post_samples_genpref_freq)
print(post_samples_relfreq_freq)
```

## Plots

```{r}
gpt2_df = data.frame(fixef(gpt2_2afc_model_just_binoms))%>%
  mutate(model = 'gpt-2') %>%
  rownames_to_column('Beta Coefficient') %>%
  mutate(Intercept = Estimate[1])

gpt2xl_df = data.frame(fixef(gpt2xl_2afc_model_just_binoms)) %>%
  mutate(model = 'gpt-2 XL') %>%
  rownames_to_column('Beta Coefficient') %>%
  mutate(Intercept = Estimate[1])

llama7b_df = data.frame(fixef(llama7b_2afc_model_just_binoms)) %>%
  mutate(model = 'Llama-2 7B') %>%
  rownames_to_column('Beta Coefficient') %>%
  mutate(Intercept = Estimate[1])

llama13b_df = data.frame(fixef(llama13b_2afc_model_just_binoms)) %>%
  mutate(model = 'Llama-2 13B') %>%
  rownames_to_column('Beta Coefficient') %>%
  mutate(Intercept = Estimate[1])


all_models_data = gpt2_df %>%
  full_join(gpt2xl_df) %>%
  full_join(llama7b_df) %>%
  full_join(llama13b_df)


gen_pref_plot_data = all_models_data %>%
  filter(`Beta Coefficient` == 'GenPref')

observed_pref_plot_data = all_models_data %>%
  filter(`Beta Coefficient` == 'RelFreq')

gen_pref_plot_data$model = factor(gen_pref_plot_data$model, levels = c('gpt-2', 'gpt-2 XL', 'Llama-2 7B', 'Llama-2 13B'))

observed_pref_plot_data$model = factor(observed_pref_plot_data$model, levels = c('gpt-2', 'gpt-2 XL', 'Llama-2 7B', 'Llama-2 13B'))

gen_pref_plot = ggplot(data = gen_pref_plot_data, aes(x = model, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_errorbar(width = 0.2) +
  geom_point(size = 1.5) +
  theme_bw() +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  ylim(c(-2.5, 10)) +
  ylab('Beta Coefficient for Generative Preferences') +
  xlab('Language Model')


observed_pref_plot = ggplot(data = observed_pref_plot_data, aes(x = model, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_errorbar(width = 0.2) +
  geom_point(size = 1.5) +
  theme_bw() +
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  ylim(c(-2.5, 10)) +
  ylab('Beta Coefficient for Observed Preferences') +
  xlab('Language Model')
  

gen_pref_plot
#ggsave('../Data/gen_pref_plot.png')
observed_pref_plot
#ggsave('../Data/observed_pref_plot.png')


full_plot_data = all_models_data %>%
  filter(`Beta Coefficient` != 'Intercept') %>%
  mutate(color = ifelse( `Beta Coefficient` == 'GenPref', 'red', ifelse(`Beta Coefficient` == 'RelFreq', 'blue', ifelse(`Beta Coefficient` == 'OverallFreq', 'purple', ifelse(`Beta Coefficient` == 'GenPref:OverallFreq', 'darkgreen', 'orange')))))


full_plot_data$model = factor(full_plot_data$model, levels = c('gpt-2', 'gpt-2 XL', 'Llama-2 7B', 'Llama-2 13B'))

full_plot_data$`Beta Coefficient` = factor(full_plot_data$`Beta Coefficient`, levels = c('RelFreq', 'GenPref', 'OverallFreq', 'RelFreq:OverallFreq', 'GenPref:OverallFreq'))

full_plot = ggplot(data = full_plot_data, aes(x = `Beta Coefficient`, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = color)) +
  geom_errorbar(width = 0.2) +
  geom_point(size = 1.5) +
  facet_wrap(~model, nrow = 1) +
  theme_bw() +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        axis.text.x = element_text(hjust = 0, angle = -45),
        legend.position = 'none',
        plot.margin = margin(10, 50, 10, 10),
        aspect.ratio = 2/1.5
        ) +
  scale_x_discrete(labels = c(expression('ObsPref'), expression('GenPref'), expression('Freq'), expression('ObsPref:Freq'), expression('GenPref:Freq'))) +
 # ylim(c(-2.5, 10)) +
  ylab('Estimate') +
  xlab('Beta Coefficient Variable')

full_plot

#ggsave('../Data/full_plot.png')
```

### Reweighting generative preferences

One possibility is that LLMs have learned to weight the generative constraints differently than humans. In this section, we re-weight the generative preferences per LLM preferences, and then run another set of models with the updated generative constraints.

#### Re-weighted constraints

##### GPT2

```{r}
options(contrasts = c("contr.sum","contr.sum"))


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  #prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

gpt2_data_just_binoms = gpt2_data_just_binoms %>%
  rename('Stress' = `*BStress`) %>%
  # mutate_each_(funs(factor(.)), c('Form', 'Percept', 'Culture', 'Power', 'Intense', 'Icon', 'Len', 'Lapse', 'Stress')) %>%
  mutate(AlphaPref = ifelse(`Alpha Probs` > `Nonalpha Probs`, 1, 0))

reweighted_model_gpt = brm(data = gpt2_data_just_binoms,
                       AlphaPref ~ Form + Percept + Power + Icon + Len + `Stress` + Freq,
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2_reweighted_constraints'
                      )

fixef(reweighted_model_gpt)
```

##### GPT2-XL

```{r}
options(contrasts = c("contr.sum","contr.sum"))

gpt2xl_data_just_binoms = gpt2xl_data_just_binoms %>%
  rename('Stress' = `*BStress`) %>%
  # mutate_each_(funs(factor(.)), c('Form', 'Percept', 'Culture', 'Power', 'Intense', 'Icon', 'Len', 'Lapse', 'Stress')) %>%
  mutate(AlphaPref = ifelse(`Alpha Probs` > `Nonalpha Probs`, 1, 0))

reweighted_model_gptxl = brm(data = gpt2xl_data_just_binoms,
                       AlphaPref ~ Form + Percept + Power + Icon + Len + `Stress` + Freq,
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_reweighted_constraints'
                      )

fixef(reweighted_model_gptxl)
```

##### Llama-7b

```{r}
options(contrasts = c("contr.sum","contr.sum"))

llama7b_data_just_binoms = llama7b_data_just_binoms %>%
  rename('Stress' = `*BStress`) %>%
  # mutate_each_(funs(factor(.)), c('Form', 'Percept', 'Culture', 'Power', 'Intense', 'Icon', 'Len', 'Lapse', 'Stress')) %>%
  mutate(AlphaPref = ifelse(`Alpha Probs` > `Nonalpha Probs`, 1, 0))

reweighted_model_llama7b = brm(data = llama7b_data_just_binoms,
                       AlphaPref ~ Form + Percept + Power + Icon + Len + `Stress` + Freq,
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       prior = prior_probs,
                       file = '../Data/llama7b_reweighted_constraints'
                      )

fixef(reweighted_model_llama7b)
```

##### Llama-13b

```{r}
options(contrasts = c("contr.sum","contr.sum"))

llama13b_data_just_binoms = llama13b_data_just_binoms %>%
  rename('Stress' = `*BStress`) %>%
  # mutate_each_(funs(factor(.)), c('Form', 'Percept', 'Culture', 'Power', 'Intense', 'Icon', 'Len', 'Lapse', 'Stress')) %>%
  mutate(AlphaPref = ifelse(`Alpha Probs` > `Nonalpha Probs`, 1, 0))


reweighted_model_llama13b = brm(data = llama13b_data_just_binoms,
                       AlphaPref ~ Form + Percept + Power + Icon + Len + `Stress` + Freq,
                       family = bernoulli(link = 'logit'),
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       prior = prior_probs,
                       file = '../Data/llama13b_reweighted_constraints'
                      )

fixef(reweighted_model_llama13b)
```

#### Humans vs Models

<!--# these models aren't converging, not sure what to do -->

Preparing data:

```{r}


prior_probs = c(
  prior(student_t(2, 0, 0.5), class = 'Intercept'),
  #prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(2, 0, 0.5), class = 'b'),
  prior(student_t(2, 0, 0.5), class = 'sd')
  #prior(student_t(3, 0, 1), class = 'sigma')
)



human_data = read_csv('../Data/data_2afc.csv') %>% #using the 2afc data from the preprint, will need to cite
  separate(UnorderedBinomial, sep = ',', into = c('WordA', 'WordB'), remove = F) %>%
  mutate(WordA = tolower(WordA), WordB = tolower(WordB)) %>%
  mutate(data_type = 'human') %>%
  rename(Stress = 'FinalStress') %>%
  rename(AlphaPref = 'response')

gpt2_data_just_binoms = gpt2_data_just_binoms %>%
  mutate(data_type = 'llm')
gpt2xl_data_just_binoms = gpt2xl_data_just_binoms %>%
  mutate(data_type = 'llm')
llama7b_data_just_binoms = llama7b_data_just_binoms %>%
  mutate(data_type = 'llm')
llama13b_data_just_binoms = llama13b_data_just_binoms %>%
  mutate(data_type = 'llm')

human_data_gpt2 = human_data %>%
  full_join(gpt2_data_just_binoms, by = c('WordA', 'WordB', 'AlphaPref', 'Len', 'Lapse', 'Stress', 'Freq', 'Form', 'Percept', 'Power', 'Icon', 'Culture', 'Intense', 'data_type')) %>%
  filter(WordA %in% (split(WordA, data_type) %>% reduce(intersect))) %>%
  ungroup() %>%
  group_by(WordA, WordB) %>%
  mutate(Item = cur_group_id()) %>%
  select(-item)

human_data_gpt2xl = human_data %>%
  full_join(gpt2xl_data_just_binoms, by = c('WordA', 'WordB', 'AlphaPref', 'Len', 'Lapse', 'Stress', 'Freq', 'Form', 'Percept', 'Power', 'Icon', 'Culture', 'Intense', 'data_type'))  %>%
  filter(WordA %in% (split(WordA, data_type) %>% reduce(intersect))) %>%
  ungroup() %>%
  group_by(WordA, WordB) %>%
  mutate(Item = cur_group_id()) %>%
  select(-item)

human_data_llama7b = human_data %>%
  full_join(llama7b_data_just_binoms, by = c('WordA', 'WordB', 'AlphaPref', 'Len', 'Lapse', 'Stress', 'Freq', 'Form', 'Percept', 'Power', 'Icon', 'Culture', 'Intense', 'data_type'))  %>%
  filter(WordA %in% (split(WordA, data_type) %>% reduce(intersect))) %>%
  ungroup() %>%
  group_by(WordA, WordB) %>%
  mutate(Item = cur_group_id()) %>%
  select(-item)

human_data_llama13b = human_data %>%
  full_join(llama13b_data_just_binoms, by = c('WordA', 'WordB', 'AlphaPref', 'Len', 'Lapse', 'Stress', 'Freq', 'Form', 'Percept', 'Power', 'Icon', 'Culture', 'Intense', 'data_type'))  %>%
  filter(WordA %in% (split(WordA, data_type) %>% reduce(intersect))) %>%
  ungroup() %>%
  group_by(WordA, WordB) %>%
  mutate(Item = cur_group_id()) %>%
  select(-item)
```

##### Humans

```{r}
options(contrasts = c("contr.sum","contr.sum"))

human_prefs = brm(data = human_data,
                       AlphaPref ~ (Form + Percept + Power + Icon + Len + `Stress` + Freq) + (1|item) + (1|subj),
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/human_prefs'
                      )

fixef(human_prefs)
```

Quick plot of human vs LLM results:

```{r}

beta_coef_labels = c('Intercept', 'Form', 'Freq', 'Icon', 'Len', 'Percept', 'Power', 'Stress')
levels = c('b_Intercept', 'b_Form', 'b_Freq', 'b_Icon', 'b_Len', 'b_Percept', 'b_Power', 'b_Stress')


human_data_coefs_plot = human_prefs %>%
  gather_draws(b_Intercept, b_Form, b_Percept, b_Power, b_Icon, b_Len, b_Stress, b_Freq) %>%
  ggplot(aes(x = .value, y = factor(.variable, level = levels))) +
  stat_halfeye() +
  xlab('Posterior Distribution') +
  ylab('Beta Coefficent for each fixed-effect') +
  scale_y_discrete(labels = beta_coef_labels) + 
  theme_bw()

reweighted_model_llama13b_plot = reweighted_model_llama13b %>%
  gather_draws(b_Intercept, b_Form, b_Percept, b_Power, b_Icon, b_Len, b_Stress, b_Freq) %>%
  ggplot(aes(x = .value, y = factor(.variable, level = levels))) +
  stat_halfeye() +
  xlab('Posterior Distribution') +
  ylab('Beta Coefficent for each fixed-effect') +
  scale_y_discrete(labels = beta_coef_labels) + 
  theme_bw()

ggarrange(human_data_coefs_plot, reweighted_model_llama13b_plot, nrow = 1)
```

##### GPT2

```{r}
options(contrasts = c("contr.sum","contr.sum"))

human_llm_gpt2 = brm(data = human_data_gpt2,
                       AlphaPref ~ (Form + Percept + Power + Icon + Len + `Stress` + Freq) * data_type + (1|data_type) + (1|Item),
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       control = list(max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/human_llm_gpt2'
                      )

fixef(human_llm_gpt2)
```

##### GPT2-XL

```{r}
options(contrasts = c("contr.sum","contr.sum"))

human_llm_gpt2xl = brm(data = human_data_gpt2xl,
                       AlphaPref ~ (Form + Percept + Power + Icon + Len + `Stress` + Freq) * data_type + (1 + Item|data_type) + (1|Item),
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/human_llm_gpt2xl'
                      )

fixef(human_llm_gpt2xl)
```

##### Llama-7b

```{r}
options(contrasts = c("contr.sum","contr.sum"))

human_llm_llama7b = brm(data = human_data_llama7b,
                       AlphaPref ~ (Form + Percept + Power + Icon + Len + `Stress` + Freq) * data_type + (1 + Item|data_type) + (1|Item),
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 20),
                       file = '../Data/human_llm_llama7b'
                      )

fixef(human_llm_llama7b)
```

##### Llama-13b

```{r}
options(contrasts = c("contr.sum","contr.sum"))

human_llm_llama13b = brm(data = human_data_llama13b,
                       AlphaPref ~ (Form + Percept + Power + Icon + Len + `Stress` + Freq) * data_type+ (1|data_type),# + (1 + Item||data_type) + (1||Item),
                       family = bernoulli(link = 'logit'),
                       #prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       prior = prior_probs,
                       #control = list(max_treedepth = 15),
                       file = '../Data/human_llm_llama13b'
                      )

fixef(human_llm_llama13b)


mcmc_plot(human_llm_llama13b, type = 'trace')
#mcmc_plot(reweighted_model_llama13b, type = 'dens')
```

## Olmo Models

### 1B Model

#### Step20000 - 86B tokens

```{python}
#from huggingface_hub import list_repo_refs

#out = list_repo_refs("allenai/OLMo-7B")
#branches = [b.name for b in out.branches]
```

```{r}
olmo1b_20000_86B = read_csv('../Data/olmo1b_step20000_tokens84B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo1b_20000_86B_model = brm(data = olmo1b_20000_86B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo1b_20000_86B'
                      )

fixef(olmo1b_20000_86B_model)
```

#### Step30000 - 126B Tokens

```{r}
olmo1b_30000_126B = read_csv('../Data/olmo1b_step30000_tokens126B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo1b_30000_126B_model = brm(data = olmo1b_30000_126B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo1b_30000_126B'
                      )

fixef(olmo1b_30000_126B_model)
```

#### Step 4000 168B Tokens

```{r}
olmo1b_40000_168B = read_csv('../Data/olmo1b_step40000_tokens168B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo1b_40000_168B_model = brm(data = olmo1b_40000_168B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo1b_40000_168B'
                      )

fixef(olmo1b_40000_168B_model)
```

#### Full Model

```{r}
olmo1b_main = read_csv('../Data/olmo1b_main.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo1b_main_model = brm(data = olmo1b_main,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo1b_main'
                      )



fixef(olmo1b_main_model)

post_samples_olmo1b = as.data.frame(fixef(olmo1b_main_model, summary = F))
post_samples_genpref = sum(post_samples_olmo1b$GenPref < 0) / length(post_samples_olmo1b$GenPref)
post_samples_genpref
```

### 7B Model

#### Step20000 - 84B tokens

```{python}
#from huggingface_hub import list_repo_refs

#out = list_repo_refs("allenai/OLMo-7B")
#branches = [b.name for b in out.branches]
```

```{r}
olmo7b_20000_86B = read_csv('../Data/olmo7b_step20000_tokens84B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo7b_20000_86B_model = brm(data = olmo7b_20000_86B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo7b_20000_86B'
                      )

fixef(olmo7b_20000_86B_model)
```

#### Step30000 - 126B Tokens

```{r}
olmo7b_30000_126B = read_csv('../Data/olmo7b_step30000_tokens128B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo7b_30000_126B_model = brm(data = olmo7b_30000_126B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo7b_30000_126B'
                      )

fixef(olmo7b_30000_126B_model)
```

#### Step 40000 168B Tokens

```{r}
olmo7b_40000_168B = read_csv('../Data/olmo7b_step40000_tokens168B.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo7b_40000_168B_model = brm(data = olmo1b_40000_168B,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo7b_40000_168B'
                      )

fixef(olmo7b_40000_168B_model)
```

#### Full Model

```{r}
olmo7b_main = read_csv('../Data/olmo7b_main.csv') %>%
  separate(binom, c('WordA', 'and', 'WordB'), remove = F, sep = ' ') %>%
  select(-and) %>%
  mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(log_freq = log(OverallFreq)) %>%
  mutate(OverallFreq = log_freq - mean(log_freq)) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  mutate(RelFreq = RelFreq - 0.5)


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

options(contrasts = c("contr.sum","contr.sum"))

olmo7b_main_model = brm(data = olmo7b_main,
                       log_odds ~ GenPref + RelFreq + GenPref:OverallFreq + RelFreq:OverallFreq + OverallFreq,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       control = list(max_treedepth = 20),
                       file = '../Data/olmo7b_main'
                      )

fixef(olmo7b_main_model)


post_samples_olmo7b = as.data.frame(fixef(olmo7b_main_model, summary = F))

post_samples_genpref = sum(post_samples_olmo7b$GenPref < 0) / length(post_samples_olmo7b$GenPref)
post_samples_genpref
```
