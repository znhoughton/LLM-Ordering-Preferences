---
`---
title: "Ordering Preferences"
author: "Zachary Houghton"
date: "2023-12-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(tidyverse)

#Sys.setenv(RETICULATE_PYTHON = "C:/Users/zacha/AppData/Local/Programs/Python/Python39/") 
#RETICULATE_PYTHON="C:/Users/zacha/AppData/Local/Programs/Python/Python39/"

use_python("C:/Users/zacha/AppData/Local/Programs/Python/Python39/python.exe")
#py_config()

#py_install(c('pytorch', 'pandas', 'numpy', 'transformers'))


```

## Ordering Preferences

In this markdown we'll be investigating phi2's ordering preferences of binomials (which we'll take from Morgan and Levy, 2016).

First we'll define our function, which gets the probability distribution for the next word in a sequence:

```{python}
#load packages
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from torch import nn
from collections import defaultdict
import pandas as pd

tokenizer = AutoTokenizer.from_pretrained("gpt2-xl") #we'll use chat-gpt2, but we could use another model if we wanted
model = AutoModelForCausalLM.from_pretrained("gpt2-xl") #load the model

def next_prob(seq, topk = True, show_probs = False):
  
  inputs = tokenizer(seq, return_tensors="pt") #tokenize our input
  prob_dist = {} #create empty dictionary
  input_ids = inputs["input_ids"]  # just IDS, no attn mask
  
  with torch.no_grad():
    logits = model(**inputs).logits[:, -1, :] #get the probability for the last word in logits
    probs = nn.functional.softmax(model(**inputs).logits[:, -1, :], dim = -1) #get the probability for the last word in probability
    
  if topk == True:
    
    #print(logits.size())
  
    pred_id = torch.topk(logits, k = 20000) #prediction in logits
    pred_id2 = torch.topk(probs, k = 20000) #prediction in probabilities
    
  else:
    
    pred_id = torch.topk(logits, k = len(logits[0]))
    pred_id2 = torch.topk(probs, k = len(probs[0]))
  
  if show_probs == False:
  
    for prob, word in zip(pred_id[0].squeeze(), pred_id[1].squeeze()): #might be a better implementation but I'm still learning pytorch
      
      #pred_id2[0] is the probability
      #pred_id2[1] is the word (encoded, so needs to be encoded)
      word_prob = prob.item() #probability/logt of word depending on which we chose
      pred_word = tokenizer.decode(word.item()).strip() #predicted word (technically might not be a word, since bpe encoding)
      prob_dist[pred_word] = word_prob #store the probability of the word in a dictionary
  
  else:
    
    for prob, word in zip(pred_id2[0].squeeze(), pred_id2[1].squeeze()): #might be a better implementation but I'm still learning pytorch
      
      #pred_id2[0] is the probability
      #pred_id2[1] is the word (encoded, so needs to be encoded)
      word_prob = prob.item() #probability/logt of word depending on which we chose
      pred_word = tokenizer.decode(word.item()).strip() #predicted word (technically might not be a word, since bpe encoding)
      prob_dist[pred_word] = word_prob #store the probability of the word in a dictionary
    
  return(prob_dist)
  #print(f"\nPredicted next word for sequence is {pred_word} with probability {word_logit}")
```

Let's get the binomials that we want to get ordering preferences for:

```{r}
binomials = read_csv('../Data/corpus.csv')

#binomials = binomials

#binomials[1,1] = 'twenties'
#binomials[1,2] = 'thirties'

binomials = binomials %>%
  mutate(binomial = paste0(WordA, ', ', WordB))

binomials_min = binomials %>%
  slice_min(OverallFreq, n = 15)

binomials_max = binomials %>%
  slice_max(OverallFreq, n = 15)

binomials_bread_and_butter = binomials %>% 
  filter(WordA == 'bread' & WordB == 'butter')

binomials_min_and_max = binomials_min %>%
  full_join(binomials_max) %>%
  full_join(binomials_bread_and_butter)

r_to_py(binomials)

r_to_py(binomials_min_and_max)

binomials
```

## Getting Ordering Preferences from gpt-2

```{python}
#print(r.binomials)

#I just want the ordering preference to contain the normalized probs, so we need to get the product of the probabilities for ordering1 and then the product of the probabilities for ordering2, then normalize them by the sum  

ordering_prefs = {}

for row in r.binomials.itertuples():
  print(row[0])
  
  worda = row[1]
  wordb = row[2]
  
  #worda = 'bread'
  #wordb = 'butter'
  
  ordering1 = worda + ' and ' + wordb
  worda_and = worda + ' and'
  wordb_and = wordb + ' and'
  
  #print(ordering1)
  #print(worda_and)
  #print(wordb_and)
  
  try:
    word2_given_word1 = next_prob(worda_and, topk = False, show_probs = True)[wordb]
  
  except:
    
    word2_given_word1 = 'NA'
  
  try:
    word1_given_word2 = next_prob(wordb_and, topk = False, show_probs = True)[worda]
  
  except:
    
    word1_given_word2 = 'NA'
  
  try:
    
    ordering_pref_alpha_normalized = word2_given_word1 / (word2_given_word1 + word1_given_word2)
    
  except:
    
    ordering_pref_alpha_normalized = 'NA'
    
  #print(ordering_pref_alpha_normalized)
  
  ordering_prefs[ordering1] = [word2_given_word1, word1_given_word2, ordering_pref_alpha_normalized]
  
#L = [(k,x, a, b) for k, x, a, b in ordering_prefs.items().split(',')]



#ordering_prefs_df = pd.DataFrame(L, columns = ['binomial', 'p(wordb|worda and)', 'p(worda|wordb and', 'normalized probability of alpha ordering'])

ordering_prefs_df = pd.DataFrame.from_dict(ordering_prefs, orient = 'index', columns = ['p(wordb|worda and)', 'p(worda|wordb and', 'normalized probability of alpha ordering'])

```

Note that these probabilities are only comparing p(butter \| bread and) to p(bread \| butter and). Thus they're not raw frequencies of the binomials (as was the case with Morgan and Levy).

Now let's port it into R:

```{r}
ordering_prefs = py$ordering_prefs_df

write_csv(ordering_prefs, 'ordering_prefs_LLM.csv')
```

## Getting Ordering Preferences from Phi 2

Get our model and tokenizer:

```{python message = F}
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

#torch.set_default_device("cuda")
#torch.cuda.set_device(0)
device = torch.device("cuda:0")

#model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype="auto", trust_remote_code=True)

#model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype=torch.float32, device_map="cuda", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype=torch.float32, device_map="cpu", trust_remote_code=True)


tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", trust_remote_code=True)


```

Now we elicit ordering preferences:

```{python}

for i in range(0,10): #10 sims
  inputs = tokenizer('''
what is more natural, "bread and butter" or "butter and bread"?
    ''', return_tensors="pt", return_attention_mask=False)
  
  outputs = model.generate(**inputs, max_length=300)
  
  
  text = tokenizer.batch_decode(outputs)[0]
  print(text)
```

testing it another way:

```{python}
from huggingface_hub import snapshot_download
model_path = snapshot_download(repo_id="amgadhasan/phi-2",repo_type="model", local_dir="./phi-2", local_dir_use_symlinks=False)


```

```{python}
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

prompt = 'Choose the ordering of the following words, combining them with "and": wug, blicket; butter, bread'
# We need to trust remote code since this hasn't been integrated in transformers as of version 4.35
# We need to set the torch dtype globally since this model class doesn't accept dtype as argument
torch.set_default_dtype(torch.float16)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True)

def generate(prompt: str, generation_params: dict = {"max_length":200})-> str : 
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, **generation_params)
    completion = tokenizer.batch_decode(outputs)[0]
    return completion

result = generate(prompt)
result
```

## Getting Ordering Preferences from Chat-gpt

```{python}
#from typing_extensions import override
import pandas as pd
from openai import OpenAI
client = OpenAI()
import openai

gpt_assistant_prompt = "You are a helpful chat assistant."

temperature=0.2
max_tokens=256
frequency_penalty=0.0

all_sims = []
df = pd.DataFrame()

list_of_binoms = r.binomials_min_and_max['binomial']

for i in range(0,100): #1000 simulations
  if i % 20 == 0:
    print(i)
  binoms_response = []
  #for i in 30:
  for i in range(0, len(list_of_binoms)):
    
    binom = str(list_of_binoms[i])
    
    instructions = "Order the pairs of words naturally (combining them with 'and'). For example, if I give you the words 'blick' and 'wug', you would say either 'wug and blick' or 'blick and wug' (whichever you think is more natural). Your ordering should reflect the way that humans order combinations. Include only the ordering and nothinge else:"
    
    instructions_plus_binom = instructions + '\n ' + binom
    
    gpt_prompt = gpt_assistant_prompt, instructions_plus_binom
    
    message=[{"role": "assistant", "content": gpt_assistant_prompt}, {"role": "user", "content": instructions_plus_binom}]
    
    response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages = message,
    temperature=temperature,
    max_tokens=max_tokens,
      frequency_penalty=frequency_penalty
    )
      
    text = [response.choices[0].message.content]
    #print(text)
    
    binoms_response += text
      
  all_sims.append(binoms_response)

#all_sims
df = pd.DataFrame(all_sims)
#df = df.iloc[1:]
#df

#gpt_assistant_prompt = "You are an expert at English and good at providing human-like responses."

#instructions = "Order the pairs of words the way you think a human would (combining them with 'and'). For example, if I give you the words 'blick' and 'wug', you would say either 'wug and blick' or 'blick and wug' (whichever you think is more natural):"
#instructions_plus_binom = instructions + '\n ' + 'butter, bread'
    
#gpt_prompt = gpt_assistant_prompt, instructions_plus_binom
    
#message=[{"role": "assistant", "content": gpt_assistant_prompt}, {"role": "user", "content": instructions_plus_binom}]

#response = client.chat.completions.create(
#model="gpt-4",
#messages = message,
#temperature=temperature,
#max_tokens=max_tokens,
  #frequency_penalty=frequency_penalty
#)
  
#text = [response.choices[0].message.content]
#text
```

```{r}
ordering_prefs_gpt3.5 = py$df

write_csv(ordering_prefs_gpt3.5, 'ordering_prefs_gpt3.5_LLM.csv')
```

Let's get a .csv file summarizing the ordering prefs:

```{r}
# Function to calculate percentage for each value in a column
ordering_prefs = read_csv('ordering_prefs_gpt3.5_LLM.csv')

calculate_percentages = function(column_values) {
  counts = table(column_values)
  percentages = counts / sum(counts) * 100
  return(percentages)
}

ordering_prefs = ordering_prefs %>%
  arrange(across(everything()))

# Apply the function to each column in the data frame
percentage_columns <- apply(ordering_prefs, 2, calculate_percentages)

# Combine the results into a new data frame
percentage_df <- data.frame(percentage_columns)

# Print the result
print(percentage_df)

percentage_df = percentage_df %>%
  mutate_if(is.factor, as.character)

percentage_df = percentage_df[1,]

df_pairs <- percentage_df %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = ".value", names_pattern = "(column_values|Freq)")
```

## Alternate Instructions Wording

This time let's give chat-gpt a forced-choice prompt and see if that changes anything.

```{python}
import pandas as pd
from openai import OpenAI
client = OpenAI()
import numpy as np
import random
#import openai

gpt_assistant_prompt = "You are a helpful chat assistant."

temperature=0.2
max_tokens=256
frequency_penalty=0.0

all_sims = []
df = pd.DataFrame()

list_of_binoms = r.binomials_min_and_max

for i in range(0,100): #100 sims cuz I'm not rich
  if i % 20 == 0:
    print(i)
  binoms_response = []
  #for i in 30:
  for i in range(0, len(list_of_binoms['WordA'])):
    
    counterbalance = random.randint(0, 1)
    if counterbalance == 0:
      order1 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
      order2 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
    else:
      order1 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
      order2 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
    
    instructions = "I'm gonna give you two orderings of a pair of words, and I want you to choose whichever one is more natural to you.  Only include the ordering and no other words:"
    
    instructions_plus_binom = instructions + '\n ' + '"' + order1 + '"' + ' or ' + '"' + order2 + '"'
    
    gpt_prompt = gpt_assistant_prompt, instructions_plus_binom
    
    message=[{"role": "assistant", "content": gpt_assistant_prompt}, {"role": "user", "content": instructions_plus_binom}]
    
    response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages = message,
    temperature=temperature,
    max_tokens=max_tokens,
      frequency_penalty=frequency_penalty
    )
      
    text = [response.choices[0].message.content]
    #print(text)
    
    binoms_response += text
      
  all_sims.append(binoms_response)

#all_sims
all_sims_modified = [[word.strip('"') for word in inner_list] for inner_list in all_sims] #remove any quotes around the words

all_sims_modified = [[word.lower() for word in inner_list] for inner_list in all_sims_modified]

#create the dataframe:
column_names = [f'Binomial_{i+1}' for i in range(len(all_sims_modified[0]))]
#df = pd.DataFrame(all_sims_modified, columns = column_names)
#df = df.iloc[1:]
#print(df)
#response
```

```{r}
#ordering_prefs_gpt3.5_alternate = py$df
#py$column_names
ordering_prefs_gpt3.5_alternate = t(data.frame(py$all_sims_modified, row.names = NULL))

rownames(ordering_prefs_gpt3.5_alternate) = NULL
colnames(ordering_prefs_gpt3.5_alternate) = py$column_names

ordering_prefs_gpt3.5_alternate = as.data.frame(ordering_prefs_gpt3.5_alternate)

write_csv(ordering_prefs_gpt3.5_alternate, 'ordering_prefs_gpt3.5_LLM_alternate.csv')
```

And we get it into a format we can work with:

```{r}
# Function to calculate percentage for each value in a column
ordering_prefs = read_csv('ordering_prefs_gpt3.5_LLM_alternate.csv')

calculate_percentages = function(column_values) {
  counts = table(column_values)
  percentages = counts / sum(counts) * 100
  return(percentages)
}
ordering_prefs = ordering_prefs %>%
  mutate_all(~str_replace_all(., "\\.", ""))



#ordering_prefs %>% 
 # mutate(
  #  across(
   #   .cols = everything(),
    #  .fns = ~ str_replace_all(
     #   string = ..1, 
      #  pattern = '\\.', 
       # replacement = ""
      #)
    #)
  #)

ordering_prefs = ordering_prefs %>%
  arrange(across(everything()))

# Apply the function to each column in the data frame
percentage_columns <- apply(ordering_prefs, 2, calculate_percentages)

# Combine the results into a new data frame
percentage_df <- data.frame(percentage_columns)

# Print the result
print(percentage_df)

percentage_df = percentage_df %>%
  mutate_if(is.factor, as.character)

percentage_df = percentage_df[1,]

df_pairs <- percentage_df %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = ".value", names_pattern = "(column_values|Freq)")

write_csv(df_pairs, "ordering_prefs_percentages_gpt3.5_alternate.csv")
```

## Replicating the results from 3.5 Turbo with chat-gpt4

```{python}
import pandas as pd
from openai import OpenAI
client = OpenAI()
import numpy as np
import random
#import openai

gpt_assistant_prompt = "You are a helpful chat assistant."

temperature=0.2
max_tokens=256
frequency_penalty=0.0

all_sims = []
df = pd.DataFrame()

list_of_binoms = r.binomials_min_and_max

for i in range(0,100): #100 sims cuz I'm not rich
  if i % 20 == 0:
    print(i)
  binoms_response = []
  #for i in 30:
  for i in range(0, len(list_of_binoms['WordA'])):
    #counterbalancing the order so that chatgpt doesn't always get the same ordering.
    counterbalance = random.randint(0, 1)
    if counterbalance == 0:
      order1 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
      order2 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
    else:
      order1 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
      order2 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
    
    instructions = "I'm gonna give you two orderings of a pair of words, and I want you to choose whichever one is more natural to you.  Only include the ordering and no other words:"
    
    instructions_plus_binom = instructions + '\n ' + '"' + order1 + '"' + ' or ' + '"' + order2 + '"'
    
    gpt_prompt = gpt_assistant_prompt, instructions_plus_binom
    
    message=[{"role": "assistant", "content": gpt_assistant_prompt}, {"role": "user", "content": instructions_plus_binom}]
    
    response = client.chat.completions.create(
    model="gpt-4",
    messages = message,
    temperature=temperature,
    max_tokens=max_tokens,
      frequency_penalty=frequency_penalty
    )
      
    text = [response.choices[0].message.content]
    #print(text)
    
    binoms_response += text
      
  all_sims.append(binoms_response)

#all_sims
all_sims_modified = [[word.strip('"') for word in inner_list] for inner_list in all_sims] #remove any quotes around the words

all_sims_modified = [[word.lower() for word in inner_list] for inner_list in all_sims_modified]

#create the dataframe:
column_names = [f'Binomial_{i+1}' for i in range(len(all_sims_modified[0]))]
#df = pd.DataFrame(all_sims_modified, columns = column_names)
#df = df.iloc[1:]
#print(df)
#response
#all_sims_modified
```

```{r}
ordering_prefs_gpt4_alternate = t(data.frame(py$all_sims_modified, row.names = NULL))

rownames(ordering_prefs_gpt4_alternate) = NULL
colnames(ordering_prefs_gpt4_alternate) = py$column_names

ordering_prefs_gpt4_alternate = as.data.frame(ordering_prefs_gpt4_alternate)

write_csv(ordering_prefs_gpt4_alternate, 'ordering_prefs_gpt4_LLM_alternate.csv')
```

```{r}
# Function to calculate percentage for each value in a column
ordering_prefs = read_csv('ordering_prefs_gpt4_LLM_alternate.csv')

calculate_percentages = function(column_values) {
  counts = table(column_values)
  percentages = counts / sum(counts) * 100
  return(percentages)
}

ordering_prefs = ordering_prefs %>%
  mutate_all(~str_replace_all(., "\\.", ""))

ordering_prefs = ordering_prefs %>%
  arrange(across(everything()))

# Apply the function to each column in the data frame
percentage_columns <- apply(ordering_prefs, 2, calculate_percentages)

# Combine the results into a new data frame
percentage_df <- data.frame(percentage_columns)

# Print the result
print(percentage_df)

percentage_df = percentage_df %>%
  mutate_if(is.factor, as.character)

percentage_df = percentage_df[1,]

df_pairs <- percentage_df %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = ".value", names_pattern = "(column_values|Freq)")

write_csv(df_pairs, "ordering_prefs_percentages_gpt4_alternate.csv")
```

## Same as above but with 1000sims

The API for gpt3.5-turbo is cheap enough that we can replicate above with 1000sims.



```{python}
import pandas as pd
from openai import OpenAI
client = OpenAI()
import numpy as np
import random
#import openai

gpt_assistant_prompt = "You are a helpful chat assistant."

temperature=0.2
max_tokens=256
frequency_penalty=0.0

all_sims = []
df = pd.DataFrame()

list_of_binoms = r.binomials_min_and_max

for i in range(0,1000): #1000 sims since the gpt3.5 api isn't too bad
  if i % 50 == 0:
    print(i)
  binoms_response = []
  #for i in 30:
  for i in range(0, len(list_of_binoms['WordA'])):
    
    counterbalance = random.randint(0, 1)
    if counterbalance == 0:
      order1 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
      order2 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
    else:
      order1 = list_of_binoms['WordB'][i] + ' and ' + list_of_binoms['WordA'][i]
      order2 = list_of_binoms['WordA'][i] + ' and ' + list_of_binoms['WordB'][i]
    
    instructions = "I'm gonna give you two orderings of a pair of words, and I want you to choose whichever one is more natural to you.  Only include the ordering and no other words:"
    
    instructions_plus_binom = instructions + '\n ' + '"' + order1 + '"' + ' or ' + '"' + order2 + '"'
    
    gpt_prompt = gpt_assistant_prompt, instructions_plus_binom
    
    message=[{"role": "assistant", "content": gpt_assistant_prompt}, {"role": "user", "content": instructions_plus_binom}]
    
    response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages = message,
    temperature=temperature,
    max_tokens=max_tokens,
      frequency_penalty=frequency_penalty
    )
      
    text = [response.choices[0].message.content]
    #print(text)
    
    binoms_response += text
      
  all_sims.append(binoms_response)

#all_sims
all_sims_modified = [[word.strip('"') for word in inner_list] for inner_list in all_sims] #remove any quotes around the words

all_sims_modified = [[word.lower() for word in inner_list] for inner_list in all_sims_modified]

#create the dataframe:
column_names = [f'Binomial_{i+1}' for i in range(len(all_sims_modified[0]))]
#df = pd.DataFrame(all_sims_modified, columns = column_names)
#df = df.iloc[1:]
#print(df)
#response
```

```{r}
#ordering_prefs_gpt3.5_alternate = py$df
#py$column_names
ordering_prefs_gpt3.5_alternate = t(data.frame(py$all_sims_modified, row.names = NULL))

rownames(ordering_prefs_gpt3.5_alternate) = NULL
colnames(ordering_prefs_gpt3.5_alternate) = py$column_names

ordering_prefs_gpt3.5_alternate = as.data.frame(ordering_prefs_gpt3.5_alternate)

write_csv(ordering_prefs_gpt3.5_alternate, 'ordering_prefs_gpt3.5_LLM_alternate_1000sims.csv')
```

And we get it into a format we can work with:

```{r}
# Function to calculate percentage for each value in a column
ordering_prefs = read_csv('ordering_prefs_gpt3.5_LLM_alternate_1000sims.csv')

calculate_percentages = function(column_values) {
  counts = table(column_values)
  percentages = counts / sum(counts) * 100
  return(percentages)
}
ordering_prefs = ordering_prefs %>%
  mutate_all(~str_replace_all(., "\\.", ""))



#ordering_prefs %>% 
 # mutate(
  #  across(
   #   .cols = everything(),
    #  .fns = ~ str_replace_all(
     #   string = ..1, 
      #  pattern = '\\.', 
       # replacement = ""
      #)
    #)
  #)

ordering_prefs = ordering_prefs %>%
  arrange(across(everything()))

# Apply the function to each column in the data frame
percentage_columns <- apply(ordering_prefs, 2, calculate_percentages)

# Combine the results into a new data frame
percentage_df <- data.frame(percentage_columns)

# Print the result
print(percentage_df)

percentage_df = percentage_df %>%
  mutate_if(is.factor, as.character)

percentage_df = percentage_df[1,]

df_pairs <- percentage_df %>%
  mutate(across(everything(), as.character)) %>%
  pivot_longer(everything(), names_to = ".value", names_pattern = "(column_values|Freq)")

write_csv(df_pairs, "ordering_prefs_percentages_gpt3.5_alternate_1000sims.csv")
```


