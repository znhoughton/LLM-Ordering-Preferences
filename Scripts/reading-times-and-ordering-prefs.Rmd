---
title: "Reading Times and 2AFC Preferences M&L"
author: "Zachary Houghton"
date: "2024-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reticulate)
```

# Overview

We'll be calculating the 2AFC and reading times for various sentences containing binomials (taken from a combination of different M&L papers).

For calculating 2AFC, we will get the probability of one sentence and then the other by adding the log-probability for each upcoming word.

For reading time calculations, we'll do something similar but for a specific region.

# Loading Data

```{r}
data2 = read_csv('../Data/corpus.csv')
data = read_csv('../Data/materials.csv') %>%
  left_join(data2, by = c('WordA', 'WordB'))
data_novel = read_csv('../Data/corpus_novel.csv') %>%
  filter(`Experimental sentence` != 'x') %>%
  select(WordA, WordB, AlphaN, Freq, OverallFreq, RelFreq, `Experimental sentence`, `...21`, `...22`, model.prop) %>%
  rename('Sentence (WordA and WordB)' = `...21`, 'Sentence (WordB and WordA)' = `...22`, "Sentence" = `Experimental sentence`, 'GenPref' = model.prop) %>%
  mutate(CollegeFreq = OverallFreq / 323592921465 * 350000000)

data_for_analysis = data %>%
  full_join(data_novel)

binomial_alpha = data_for_analysis$`Sentence (WordA and WordB)`[6:11]
binomial_alpha2 = data_for_analysis$`Sentence (WordA and WordB)`[1:5]
binomial_nonalpha = data_for_analysis$`Sentence (WordB and WordA)`
```

# GPTXL

## 2AFC

Here's our function to get the probability of the next word in the sentence:

```{python}
#load packages
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from torch import nn
#from collections import defaultdict
import pandas as pd
import re

tokenizer = AutoTokenizer.from_pretrained("gpt2-xl") #we'll use chat-gpt2, but we could use another model if we wanted
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("gpt2-xl", return_dict_in_generate=True) #load the model
model.config.pad_token_id = model.config.eos_token_id

```

Courtesy of this thread: <https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/17>

```{python}
from pprint import pprint
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

def to_tokens_and_logprobs(model, tokenizer, input_texts):
  

    input_ids = tokenizer(input_texts, padding=True, return_tensors="pt").to(device).input_ids
  
    outputs = model(input_ids)
    probs = torch.log_softmax(outputs.logits, dim=-1).detach()

    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1
    probs = probs[:, :-1, :]
    input_ids = input_ids[:, 1:]
    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)
    
    batch = []
    for input_sentence, input_probs in zip(input_ids, gen_probs):
        text_sequence = []
        for token, p in zip(input_sentence, input_probs):
            if token not in tokenizer.all_special_ids:
                text_sequence.append((tokenizer.decode(token), p.item()))
        batch.append(text_sequence)
    return batch


#input_texts = ["The boy went outside to fly his kite.", "this is a test."]

#batch = to_tokens_and_logprobs(model, tokenizer, input_texts)
#sentence_probs = [sum(item[1] for item in inner_list) for inner_list in batch]

#binom_probs = {}


#pprint(batch)
```

Now we'll apply this to each sentence:

```{python}
#R keeps crashing, maybe we need to run in smaller chunks

input_texts_alpha = r.binomial_alpha
input_texts_nonalpha = r.binomial_nonalpha

n_batches = len(input_texts) / 100
input_texts_alpha = (np.array(np.array_split(np.array(input_texts_alpha), n_batches))).tolist() #for some reason, I can't conver the np array to a list -- python seems to think it's already a list, so I force it to be an array then conver it to a list

input_texts_nonalpha = (np.array(np.array_split(np.array(input_texts_nonalpha), n_batches))).tolist() 

batch_alpha = [[]]
for minibatch in input_texts_alpha:
  batch_placeholder = to_tokens_and_logprobs(model, tokenizer, minibatch)
  batch_alpha.extend(batch_placeholder)
  
batch_nonalpha = [[]]
for minibatch in input_texts_nonalpha:
  batch_placeholder = to_tokens_and_logprobs(model, tokenizer, minibatch)
  batch_nonalpha.extend(batch_placeholder)
  
batch_alpha = batch_alpha[1:]
batch_nonalpha = batch_nonalpha[1:]

#batch_alpha = to_tokens_and_logprobs(model, tokenizer, input_texts_alpha)
#batch_nonalpha = to_tokens_and_logprobs(model, tokenizer, input_texts_nonalpha)

sentence_probs_alpha = [sum(item[1] for item in inner_list) for inner_list in batch_alpha]
sentence_probs_nonalpha = [sum(item[1] for item in inner_list) for inner_list in batch_nonalpha]


#count = 0
#for item in batch_alpha[1]: 
    #count += item[1]

for i,row in enumerate(r.data_for_analysis.itertuples()):
  binom = row[2] + ' and ' + row[3]
  binom_probs[binom] = [sentence_probs_alpha[i], sentence_probs_nonalpha[i]]


binom_probs_df = pd.DataFrame.from_dict(binom_probs, orient = 'index', columns = ['Alpha Probs', 'Nonalpha Probs'])
binom_probs_df.reset_index(inplace=True)
binom_probs_df.rename(columns = {'index': 'binom'}, inplace = True)
#pprint(batch_alpha)
#pprint(batch_nonalpha)
```

```{r}
binom = py$binom_probs_df

batch_alpha_gpt2xl = py$batch_alpha
batch_nonalpha_gpt2xl = py$batch_nonalpha

write_csv(batch_alpha_gpt2xl, '../Data/gpt2xl_batch_alpha.csv')
write_csv(batch_nonalpha_gpt2xl, '../Data/gpt2xl_batch_nonalpha.csv')

binom = binom %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`)))
write_csv(binom, '../Data/gpt2xl_2afc_ordering_prefs.csv')
```

## Reading Times

We can do largely the same procedure for reading times, but we want to take into account the spillover region. So instead of looking at the entire sentence (as in the 2afc), we will look at the region that contains the first binomial and the 5 following words.

```{r}
r_to_py(data_for_analysis)
```

First let's get the probabilities and words for each sentence as columns in the df:

```{python}
reading_time_data = r.data_for_analysis

second_item_lists = [[item[1] for item in sublist] for sublist in batch_alpha]
second_item_lists_nonalpha = [[item[1] for item in sublist] for sublist in batch_nonalpha]
word_list = [[item[0] for item in sublist] for sublist in batch_alpha]
word_list_nonalpha = [[item[0] for item in sublist] for sublist in batch_nonalpha]
new_df = pd.DataFrame({'Second_Item_List': second_item_lists, 'Word_list': word_list, 'Second_Item_List_nonalpha': second_item_lists_nonalpha, 'Word_list_nonalpha': word_list_nonalpha})
reading_time_data = pd.concat([reading_time_data, new_df], axis = 1)




```

We need to get a bit creative for the next part:

If items are split within word, they don't start with a space. So items starting with a space are a whole word. 'and' is also always a whole word, so we can index where ' and' is and find the closest word starting with a space that comes before ' and'.

We'll do this by:

-finding the position of ' and' (if there are multiple 'ands' we choose the one that comes immediately after one of the binomial words. There's a slim chance this will break if there are multiple ands AND the word in the binomial is tokenized into subtokens. However, this is probably pretty unlikely).

-finding the positions of words that begin with a space

-find the largest position starting with a space that is still smaller than the position of ' and'

```{python}
def find_and_index(words, word_alpha):
    positions = []
    found_word_a = False

    for i, word in enumerate(words):
        if word == ' and' and found_word_a:
            positions.append(i)
            found_word_a = False
        elif word.strip(' ') == word_alpha:
            found_word_a = True
        else:
            found_word_a = False
    
    if positions == []:
        positions.append(words.index(' and'))
        
    return positions[0]
      
# Initialize an empty list to store the positions
positions = []

# Iterate through rows and words
for row_index, row in enumerate(reading_time_data['Word_list']):
    word_positions = []
    for word_index, word in enumerate(row):
        if word.startswith(' '):
            word_positions.append(word_index)
    positions.append(word_positions)

reading_time_data['alpha_space_conditions'] = positions

positions = []
for row_index, row in enumerate(reading_time_data['Word_list_nonalpha']):
    word_positions = []
    for word_index, word in enumerate(row):
        if word.startswith(' '):
            word_positions.append(word_index)
    positions.append(word_positions)
    
# Create a new column 'position' in the DataFrame

reading_time_data['nonalpha_space_conditions'] = positions


# Apply the function to create a new column 'and_index'
reading_time_data['and_index'] = reading_time_data.apply(lambda row: find_and_index(row['Word_list'], row['WordA']), axis=1)
#reading_time_data['and_index'] = reading_time_data['Word_list'].apply(find_and_index, 'WordA')

#def num_tokens(word):
  #tokens_in_word = tokenizer(word, return_tensors="pt").input_ids
  #return tokens_in_word.numel()


# Define a custom function to find the largest number in the list that is less than the comparison number
def find_largest_less_than(nums, comparison_num):
    valid_nums = [num for num in nums if num < comparison_num]
    if valid_nums:
        return max(valid_nums)
    else:
        return None

# Apply the custom function to create a new column 'largest_less_than'
reading_time_data['largest_less_than_alpha'] = reading_time_data.apply(lambda row: find_largest_less_than(row['alpha_space_conditions'], row['and_index']), axis=1)

reading_time_data['largest_less_than_nonalpha'] = reading_time_data.apply(lambda row: find_largest_less_than(row['nonalpha_space_conditions'], row['and_index']), axis=1)
  

#reading_time_data['alpha_diff'] = reading_time_data['largest_less_than_alpha'] - reading_time_data['and_index']
#reading_time_data['alpha_bpe_num'] = reading_time_data['WordA'].apply(num_tokens)
#reading_time_data['nonalpha_bpe_num'] = reading_time_data['WordB'].apply(num_tokens)
```

Finally let's extract the numbers for the 6-word region we care about:

<!--# Flaw with this: it's actually a 6-bpe token region, not a 6-word region -- prob worth discussing -->

```{python}
def extract_number(row):
    index = row['largest_less_than_alpha'] 
    number_list = row['Second_Item_List']
    
    # Check if the index is valid
    if 0 <= index < len(number_list):
        return number_list[index:index+6]
    else:
        return None  # Handle cases where the index is out of bounds
      
def extract_number_nonalpha(row):
    index = row['largest_less_than_nonalpha']
    number_list = row['Second_Item_List_nonalpha']
    
    # Check if the index is valid
    if 0 <= index < len(number_list):
        return number_list[index:index+6]
    else:
        return None  # Handle cases where the index is out of bounds

# Apply the function to create the 'extracted_number_column'
reading_time_data['extracted_number_column_alpha'] = reading_time_data.apply(extract_number, axis=1)
reading_time_data['extracted_number_column_nonalpha'] = reading_time_data.apply(extract_number_nonalpha, axis = 1)

def sum_numbers(row):
  return sum(row)

def word_region_alpha(row):
  index = row['largest_less_than_alpha']
  word_list = row['Word_list']
  word_list = word_list[index:index+6]
  return word_list

#def word_region_nonalpha(row)

reading_time_data['reading_times_alpha'] = reading_time_data['extracted_number_column_alpha'].apply(sum_numbers)
reading_time_data['reading_times_nonalpha'] = reading_time_data['extracted_number_column_nonalpha'].apply(sum_numbers)

reading_time_data['subset_words_alpha'] = reading_time_data.apply(word_region_alpha, axis = 1)
```

```{python}
columns_to_keep = ['WordA', 'WordB', 'Sentence', 'reading_times_alpha', 'reading_times_nonalpha']
reading_time_data = reading_time_data[columns_to_keep]
```

```{r}
reading_time_data = py$reading_time_data

write_csv(reading_time_data, '../Data/gpt2xl_reading_time_data.csv')
```

<!--# to-do: join them all into one dataframe -->

Let's do some checking by hand to make sure everything works.

# GPT2 (not xl)

## 2afc

```{python}
#load packages
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from torch import nn
from collections import defaultdict
import pandas as pd
import re

tokenizer = AutoTokenizer.from_pretrained("gpt2") #we'll use chat-gpt2, but we could use another model if we wanted
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("gpt2", return_dict_in_generate=True) #load the model
model.config.pad_token_id = model.config.eos_token_id

```

Courtesy of this thread: <https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/17>

```{python}
from pprint import pprint
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def to_tokens_and_logprobs(model, tokenizer, input_texts):
    input_ids = tokenizer(input_texts, padding=True, return_tensors="pt").input_ids
    
    outputs = model(input_ids)
    probs = torch.log_softmax(outputs.logits, dim=-1).detach()

    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1
    probs = probs[:, :-1, :]
    input_ids = input_ids[:, 1:]
    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)

    batch = []
    for input_sentence, input_probs in zip(input_ids, gen_probs):
        text_sequence = []
        for token, p in zip(input_sentence, input_probs):
            if token not in tokenizer.all_special_ids:
                text_sequence.append((tokenizer.decode(token), p.item()))
        batch.append(text_sequence)
    return batch


#input_texts = ["The boy went outside to fly his kite.", "this is a test."]

#batch = to_tokens_and_logprobs(model, tokenizer, input_texts)
#sentence_probs = [sum(item[1] for item in inner_list) for inner_list in batch]

#binom_probs = {}


#pprint(batch)
```

Now we'll apply this to each sentence:

```{python}
input_texts_alpha = r.binomial_alpha
input_texts_nonalpha = r.binomial_nonalpha


n_batches = len(input_texts) / 100
input_texts_alpha = (np.array(np.array_split(np.array(input_texts_alpha), n_batches))).tolist() #for some reason, I can't conver the np array to a list -- python seems to think it's already a list, so I force it to be an array then conver it to a list

input_texts_nonalpha = (np.array(np.array_split(np.array(input_texts_nonalpha), n_batches))).tolist() 

batch_alpha = [[]]
for minibatch in input_texts_alpha:
  batch_placeholder = to_tokens_and_logprobs(model, tokenizer, minibatch)
  batch_alpha.extend(batch_placeholder)
  
batch_nonalpha = [[]]
for minibatch in input_texts_nonalpha:
  batch_placeholder = to_tokens_and_logprobs(model, tokenizer, minibatch)
  batch_nonalpha.extend(batch_placeholder)
  
batch_alpha = batch_alpha[1:]
batch_nonalpha = batch_nonalpha[1:]

sentence_probs_alpha = [sum(item[1] for item in inner_list) for inner_list in batch_alpha]
sentence_probs_nonalpha = [sum(item[1] for item in inner_list) for inner_list in batch_nonalpha]
binom_probs = {}

#count = 0
#for item in batch_alpha[1]: 
    #count += item[1]

for i,row in enumerate(r.data.itertuples()):
  binom = row[1] + ' and ' + row[2]
  binom_probs[binom] = [sentence_probs_alpha[i], sentence_probs_nonalpha[i]]
binom_probs_df = pd.DataFrame.from_dict(binom_probs, orient = 'index', columns = ['Alpha Probs', 'Nonalpha Probs'])
binom_probs_df.reset_index(inplace=True)
binom_probs_df.rename(columns = {'index': 'binom'}, inplace = True)
#pprint(batch_alpha)
#pprint(batch_nonalpha)
```

```{r}
binom = py$binom_probs_df
batch_alpha_gpt2 = py$batch_alpha
batch_nonalpha_gpt2 = py$batch_nonalpha

write_csv(batch_alpha_gpt2, '../Data/gpt2_batch_alpha.csv')
write_csv(batch_nonalpha_gpt2, '../Data/gpt2_batch_nonalpha.csv')

binom = binom %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`)))
write_csv(binom, '../Data/gpt2_2afc_ordering_prefs.csv')
```

## Reading Times

We can do largely the same procedure for reading times, but we want to take into account the spillover region. So instead of looking at the entire sentence (as in the 2afc), we will look at the region that contains the first binomial and the 5 following words.

```{r}
r_to_py(data)
```

First let's get the probabilities and words for each sentence as columns in the df:

```{python}
reading_time_data = r.data

second_item_lists = [[item[1] for item in sublist] for sublist in batch_alpha]
second_item_lists_nonalpha = [[item[1] for item in sublist] for sublist in batch_nonalpha]
word_list = [[item[0] for item in sublist] for sublist in batch_alpha]
word_list_nonalpha = [[item[0] for item in sublist] for sublist in batch_nonalpha]
new_df = pd.DataFrame({'Second_Item_List': second_item_lists, 'Word_list': word_list, 'Second_Item_List_nonalpha': second_item_lists_nonalpha, 'Word_list_nonalpha': word_list_nonalpha})
reading_time_data = pd.concat([reading_time_data, new_df], axis = 1)




```

We need to get a bit creative for the next part:

If items are split within word, they don't start with a space. So items starting with a space are a whole word. 'and' is also always a whole word, so we can index where ' and' is and find the closest word starting with a space that comes before ' and'.

We'll do this by:

-finding the position of ' and' (if there are multiple 'ands' we choose the one that comes immediately after one of the binomial words. There's a slim chance this will break if there are multiple ands AND the word in the binomial is tokenized into subtokens. However, this is probably pretty unlikely).

-finding the positions of words that begin with a space

-find the largest position starting with a space that is still smaller than the position of ' and'

```{python}
def find_and_index(words, word_alpha):
    positions = []
    found_word_a = False

    for i, word in enumerate(words):
        if word == ' and' and found_word_a:
            positions.append(i)
            found_word_a = False
        elif word.strip(' ') == word_alpha:
            found_word_a = True
        else:
            found_word_a = False
    
    if positions == []:
        positions.append(words.index(' and'))
        
    return positions[0]
      
# Initialize an empty list to store the positions
positions = []

# Iterate through rows and words
for row_index, row in enumerate(reading_time_data['Word_list']):
    word_positions = []
    for word_index, word in enumerate(row):
        if word.startswith(' '):
            word_positions.append(word_index)
    positions.append(word_positions)

reading_time_data['alpha_space_conditions'] = positions

positions = []
for row_index, row in enumerate(reading_time_data['Word_list_nonalpha']):
    word_positions = []
    for word_index, word in enumerate(row):
        if word.startswith(' '):
            word_positions.append(word_index)
    positions.append(word_positions)
    
# Create a new column 'position' in the DataFrame

reading_time_data['nonalpha_space_conditions'] = positions


# Apply the function to create a new column 'and_index'
reading_time_data['and_index'] = reading_time_data.apply(lambda row: find_and_index(row['Word_list'], row['WordA']), axis=1)
#reading_time_data['and_index'] = reading_time_data['Word_list'].apply(find_and_index, 'WordA')

#def num_tokens(word):
  #tokens_in_word = tokenizer(word, return_tensors="pt").input_ids
  #return tokens_in_word.numel()


# Define a custom function to find the largest number in the list that is less than the comparison number
def find_largest_less_than(nums, comparison_num):
    valid_nums = [num for num in nums if num < comparison_num]
    if valid_nums:
        return max(valid_nums)
    else:
        return None

# Apply the custom function to create a new column 'largest_less_than'
reading_time_data['largest_less_than_alpha'] = reading_time_data.apply(lambda row: find_largest_less_than(row['alpha_space_conditions'], row['and_index']), axis=1)

reading_time_data['largest_less_than_nonalpha'] = reading_time_data.apply(lambda row: find_largest_less_than(row['nonalpha_space_conditions'], row['and_index']), axis=1)
  

#reading_time_data['alpha_diff'] = reading_time_data['largest_less_than_alpha'] - reading_time_data['and_index']
#reading_time_data['alpha_bpe_num'] = reading_time_data['WordA'].apply(num_tokens)
#reading_time_data['nonalpha_bpe_num'] = reading_time_data['WordB'].apply(num_tokens)
```

Finally let's extract the numbers for the 6-word region we care about:

<!--# Flaw with this: it's actually a 6-bpe token region, not a 6-word region -- prob worth discussing -->

```{python}
def extract_number(row):
    index = row['largest_less_than_alpha'] 
    number_list = row['Second_Item_List']
    
    # Check if the index is valid
    if 0 <= index < len(number_list):
        return number_list[index:index+6]
    else:
        return None  # Handle cases where the index is out of bounds
      
def extract_number_nonalpha(row):
    index = row['largest_less_than_nonalpha']
    number_list = row['Second_Item_List_nonalpha']
    
    # Check if the index is valid
    if 0 <= index < len(number_list):
        return number_list[index:index+6]
    else:
        return None  # Handle cases where the index is out of bounds

# Apply the function to create the 'extracted_number_column'
reading_time_data['extracted_number_column_alpha'] = reading_time_data.apply(extract_number, axis=1)
reading_time_data['extracted_number_column_nonalpha'] = reading_time_data.apply(extract_number_nonalpha, axis = 1)

def sum_numbers(row):
  return sum(row)

def word_region_alpha(row):
  index = row['largest_less_than_alpha']
  word_list = row['Word_list']
  word_list = word_list[index:index+6]
  return word_list

#def word_region_nonalpha(row)

reading_time_data['reading_times_alpha'] = reading_time_data['extracted_number_column_alpha'].apply(sum_numbers)
reading_time_data['reading_times_nonalpha'] = reading_time_data['extracted_number_column_nonalpha'].apply(sum_numbers)

reading_time_data['subset_words_alpha'] = reading_time_data.apply(word_region_alpha, axis = 1)
```

```{python}
columns_to_keep = ['WordA', 'WordB', 'Sentence', 'reading_times_alpha', 'reading_times_nonalpha']
reading_time_data = reading_time_data[columns_to_keep]
```

```{r}
reading_time_data = py$reading_time_data

write_csv(reading_time_data, '../Data/gpt2_reading_time_data.csv')
```
