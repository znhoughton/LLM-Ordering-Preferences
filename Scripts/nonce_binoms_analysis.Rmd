---
title: "nonce_binoms_analysis"
author: "Zachary Houghton"
date: "2024-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(brms)
library(purrr)
options(contrasts = c("contr.sum","contr.sum"))
```

Analaysis of ordering prefs for nonce binomials


Note that Lapse was removed for having a very high VIF.

# Load Data

```{r}

data_main_model = read_csv('../Data/allenai_OLMo-7B-0424-hf.csv') 

corpus = read_csv('../Data/nonce_binoms.csv')

data_validation = read_csv('../Data/olmo7b_prompt1_validation.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation2 = read_csv('../Data/olmo7b_prompt2_validation.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation3 = read_csv('../Data/olmo7b_prompt3_validation.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation4 = read_csv('../Data/olmo7b_prompt4_validation.csv') %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation = data_validation %>%
  rename(no_final_stress = `*BStress`)

data_validation2 = data_validation2 %>%
  rename(no_final_stress = `*BStress`)

data_validation3 = data_validation3 %>%
  rename(no_final_stress = `*BStress`)

data_validation4 = data_validation4 %>%
  rename(no_final_stress = `*BStress`)


data_validation_olmo2_1b = read_csv('../Data/olmo2_1b_prompt1_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation2_olmo2_1b = read_csv('../Data/olmo2_1b_prompt2_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation3_olmo2_1b = read_csv('../Data/olmo2_1b_prompt3_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation4_olmo2_1b = read_csv('../Data/olmo2_1b_prompt4_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation_olmo2_1b = data_validation_olmo2_1b %>%
  rename(no_final_stress = `*BStress`)

data_validation2_olmo2_1b = data_validation2_olmo2_1b %>%
  rename(no_final_stress = `*BStress`)

data_validation3_olmo2_1b = data_validation3_olmo2_1b %>%
  rename(no_final_stress = `*BStress`)

data_validation4_olmo2_1b = data_validation4_olmo2_1b %>%
  rename(no_final_stress = `*BStress`)




####


data_validation_gpt2xl = read_csv('../Data/gpt2xl_prompt1_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation2_gpt2xl = read_csv('../Data/gpt2xl_prompt2_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation3_gpt2xl = read_csv('../Data/gpt2xl_prompt3_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))

data_validation4_gpt2xl = read_csv('../Data/gpt2xl_prompt4_validation.csv') %>%
  mutate(log_odds = alpha_prob - nonalpha_prob) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))


data_validation_gpt2xl = data_validation_gpt2xl %>%
  rename(no_final_stress = `*BStress`)

data_validation2_gpt2xl = data_validation2_gpt2xl %>%
  rename(no_final_stress = `*BStress`)

data_validation3_gpt2xl = data_validation3_gpt2xl %>%
  rename(no_final_stress = `*BStress`)

data_validation4_gpt2xl = data_validation4_gpt2xl %>%
  rename(no_final_stress = `*BStress`)


###







data_main_model_validation = read_csv('../Data/olmo7b_prompt1_validation.csv')



bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
onegram_corpus_size = 1560674367427
count_and = 43378012800

bigram_counts = bigram_counts %>%
  mutate(ngram = str_replace_all(ngram, '\t', ' '))

data_main_model = data_main_model %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)


data_main_model = data_main_model %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))
  #mutate(log_freq = log(OverallFreq))# %>%
  #mutate(OverallFreq = log_freq - mean(log_freq))# %>%
  #mutate(GenPref = GenPref - 0.5) %>%
  #mutate(RelFreq = RelFreq - 0.5)

data_main_model = data_main_model %>%
  rename(no_final_stress = `*BStress`)



data_main_model = data_main_model %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
         bigram2_alpha = paste0('and ', Word2),
         bigram1_nonalpha = paste0(Word2, ' and'),
         bigram2_nonalpha = paste0('and ', Word1)
         )

data_main_model = data_main_model %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_main_model = data_main_model %>%
  mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
         bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
         log_bigram_prob_alpha = log(bigram_prob_alpha),
         log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
         log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)


# data_main_model = data_main_model %>%
#   mutate(Form = factor(Form), Percept = factor(Percept), Culture = factor(Culture), Power = factor(Power), Intense = factor(Intense), Icon = factor(Icon), Freq = as.numeric(as.character(Freq)), Len = factor(Len), Lapse = factor(Lapse), no_final_stress = factor(no_final_stress))
cor(data_main_model$log_odds, data_validation$log_odds)
cor(data_main_model$log_odds, data_validation2$log_odds)
cor(data_main_model$log_odds, data_validation3$log_odds)
cor(data_main_model$log_odds, data_validation4$log_odds)

```

## Load Data for checkpoints (not main)

```{r message = F}
data_path = "../Data"

# List all CSV files matching the desired pattern in the directory
file_list = list.files(path = data_path, pattern = "allenai_OLMo-7B-0424-hf_step.*\\.csv$", full.names = TRUE)

# Function to read a file and add the 'checkpoint' column
read_and_add_checkpoint = function(file) {
  # Extract the checkpoint from the filename
  checkpoint = str_extract(basename(file), "step[0-9]+.*(?=\\.csv)")
  
  # Read the file and add the checkpoint column
  read_csv(file) %>%
    mutate(checkpoint = checkpoint)
}

# Read all files and combine into a single data frame
combined_df = file_list %>%
  map_df(read_and_add_checkpoint) %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  rename(no_final_stress = `*BStress`) %>%
  mutate(num_tokens = str_extract(checkpoint, "(?<=tokens).*")) %>%
  mutate(n_billion_tokens = as.numeric(str_remove(num_tokens, "B"))) %>%
  arrange(n_billion_tokens) %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
         bigram2_alpha = paste0('and ', Word2),
         bigram1_nonalpha = paste0(Word2, ' and'),
         bigram2_nonalpha = paste0('and ', Word1)
         ) %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and) %>%
  mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
         bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
         log_bigram_prob_alpha = log(bigram_prob_alpha),
         log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
         log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)



combined_df = combined_df %>%
  mutate(n_billion_tokens = case_when(checkpoint == 'main' ~ 2050,
                                      checkpoint != 'main' ~ n_billion_tokens))

#write_csv(combined_df, '../Data/combined_df.csv')

data_list = combined_df %>%
  arrange(n_billion_tokens) %>%
  split(.$checkpoint)  

checkpoint_tokens_key = combined_df %>%
  group_by(checkpoint) %>%
  slice_head(n=1) %>%
  select(checkpoint, num_tokens, n_billion_tokens)

# combined_df = combined_df %>%
#   mutate(Form = factor(Form), Percept = factor(Percept), Culture = factor(Culture), Power = factor(Power), Intense = factor(Intense), Icon = factor(Icon), Freq = as.numeric(as.character(Freq)), Len = factor(Len), Lapse = factor(Lapse), no_final_stress = factor(no_final_stress))


prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)

#function to run the model for all of the checkpoints
fit_model1 = function(data, checkpoint) {
  brm(log_odds ~ GenPref,
      data = data,
      family = gaussian(),
      warmup = 2000,
      iter = 4000,
      prior = prior_probs,
      cores = 4,
      chains = 4,
      file = paste0('../Data/model1_olmo7b_', checkpoint))
}

fit_model2 = function(data, checkpoint) {
  brm(log_odds ~ GenPref * log_bigram_odds_ratio,
      data = data,
      family = gaussian(),
      warmup = 2000,
      iter = 4000,
      prior = prior_probs,
      cores = 4,
      chains = 4,
      file = paste0('../Data/model2_olmo7b_', checkpoint))
}

#function to run the second model for all of the checkpoints
#in the future, should use the update() function to avoid recompiling the model each time, might change this later when I have time
fit_model3 = function(data, checkpoint) {
  brm(log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed because it has only one value, so it is meaningless, other variables removed due to backward model selection
      data = data,
      family = gaussian(),
      warmup = 5000,
      iter = 10000,
      prior = prior_probs,
      cores = 4,
      chains = 4,
      file = paste0('../Data/model3_olmo7b_', checkpoint))
}



# Apply the model fitting function to each dataframe in the list

checkpoint_list = names(data_list)

models1 = map2(data_list, checkpoint_list, fit_model1)
models2 = map2(data_list, checkpoint_list, fit_model2)
models3 = map2(data_list, checkpoint_list, fit_model3)


# percent_greater_zero = percent_greater_zero %>%
#   arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))




extract_model_summary = function(model, checkpoint) {
  # Get the fixed effects summary
  fixef_summary = as.data.frame(fixef(model))
  fixef_summary$checkpoint = checkpoint
  percent_greater_zero = data.frame(fixef(model, summary = F)) %>%
    pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
    group_by(beta_coefficient) %>%
    summarize((sum(estimate > 0) / length(estimate)) * 100)
  
  percent_greater_zero = percent_greater_zero %>%
    arrange(match(beta_coefficient, c('Intercept', 'GenPref')))
  
  # Add rownames (parameter names) as a column
  fixef_summary$Parameter = rownames(fixef_summary)
  fixef_summary = fixef_summary %>%
    mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    rename('% Samples > 0' = `percent_greater_zero`) 
  # Reset rownames and return the result
  rownames(fixef_summary) <- NULL
  return(fixef_summary)
}


results_list1 = map2(models1, checkpoint_list, extract_model_summary)
fixefs_m1 = bind_rows(results_list1)

results_list2 = map2(models2, checkpoint_list, extract_model_summary)
fixefs_m2 = bind_rows(results_list2)

results_list3 = map2(models3, checkpoint_list, extract_model_summary)
fixefs_m3 = bind_rows(results_list3)

#write_csv(fixefs_m1, '../Data/fixefs_m1.csv')
#write_csv(fixefs_m2, '../Data/fixefs_m2.csv')
#write_csv(fixefs_m3, '../Data/fixefs_m3.csv')

```



# Main Models

## Main Model

### Model1

```{r}

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)



Olmo_main_genpref = brm(log_odds ~ GenPref,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model1_main'
                      )

fixef(Olmo_main_genpref)


Olmo_main_genpref_prompt2 = brm(log_odds ~ GenPref,
                       data = data_validation2,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_main_genpref_prompt2'
                      )

fixef(Olmo_main_genpref_prompt2)


Olmo_main_genpref_prompt3 = brm(log_odds ~ GenPref,
                       data = data_validation3,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_main_genpref_prompt3'
                      )

fixef(Olmo_main_genpref_prompt3)


Olmo_main_genpref_prompt4 = brm(log_odds ~ GenPref,
                       data = data_validation4,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_main_genpref_prompt4'
                      )

fixef(Olmo_main_genpref_prompt4)






```





### Model2

```{r}

Olmo_main_genpref_log_odds = brm(log_odds ~ GenPref * log_bigram_odds_ratio,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model2_main'
                      )

fixef(Olmo_main_genpref_log_odds)
```

### Model3
```{r}

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)
#lapse removed for having high VIF
#removed no_final_stress, intense, and percept because of backward model selection, lowest percentage of samples greater than/less than zero
#removing freq didn't affect the other variables
Olmo_main_individual_constraints = brm(data = data_main_model,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model3_main'
                      )

Olmo_prompt2_individual_constraints = brm(data = data_validation2,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_prompt2_individual_constraints'
                      )


Olmo_prompt3_individual_constraints = brm(data = data_validation3,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_prompt3_individual_constraints'
                      )


Olmo_prompt4_individual_constraints = brm(data = data_validation4,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo_prompt4_individual_constraints'
                      )


percent_greater_zeros_prompt1 = data.frame(fixef(Olmo_main_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt1 = data.frame(fixef(Olmo_main_individual_constraints)) %>%
  mutate(Prefix = 'Next item: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt1 = percent_greater_zeros_prompt1 %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt1 = fixefs_prompt1 %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt1$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)



percent_greater_zeros_prompt2 = data.frame(fixef(Olmo_prompt2_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt2 = data.frame(fixef(Olmo_prompt2_individual_constraints)) %>%
  mutate(Prefix = 'example: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt2 = percent_greater_zeros_prompt2 %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt2 = fixefs_prompt2 %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt2$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

percent_greater_zeros_prompt3 = data.frame(fixef(Olmo_prompt3_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt3 = data.frame(fixef(Olmo_prompt3_individual_constraints)) %>%
  mutate(Prefix = 'instance: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt3 = percent_greater_zeros_prompt3 %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt3 = fixefs_prompt3 %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt3$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


percent_greater_zeros_prompt4 = data.frame(fixef(Olmo_prompt4_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt4 = data.frame(fixef(Olmo_prompt4_individual_constraints)) %>%
  mutate(Prefix = 'try this: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt4 = percent_greater_zeros_prompt4 %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt4 = fixefs_prompt4 %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt4$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


fixefs_prompt1 = fixefs_prompt1 %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt2 = fixefs_prompt2 %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt3 = fixefs_prompt3 %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt4 = fixefs_prompt4 %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))



fixefs_all_prompts = fixefs_prompt1 %>%
  full_join(fixefs_prompt2) %>%
  full_join(fixefs_prompt3) %>%
  full_join(fixefs_prompt4) %>%
  select(Prefix, Coefficient, Estimate, Est.Error, `Q2.5`, `Q97.5`, `% Samples > 0`)

#write_csv(fixefs_all_prompts, '../Data/fixefs_all_prompts.csv')


fixefs = data.frame(fixef(Olmo_main_individual_constraints, summary = F)) %>%
  select(-Intercept) %>%
  summarise(across(everything(), ~ {
    med = median(.x, na.rm = TRUE)
    if (med > 0) {
      mean(.x > 0, na.rm = TRUE) # Proportion of positive values
    } else if (med < 0) {
      mean(.x < 0, na.rm = TRUE) # Proportion of negative values
    } else {
      NA # Median is exactly zero
    }
  }))

fixefs$min_col=colnames(fixefs)[apply(fixefs, 1, which.min)]
#culture is the most centered around zero, so removing this 
```






```{r}

olmo_only_length = brm(data = data_main_model,
                       log_odds ~ Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model_only_length'
                      )

fixef(olmo_only_length)
```

```{r}
Olmo_main_all_constraints = lm(log_odds ~ Percept + Culture + Power + Intense + Freq + Len + no_final_stress, data = data_main_model)

car::vif(Olmo_main_all_constraints)

```




```{r}
coeffs = as.data.frame(fixef(Olmo_main_individual_constraints, summary = F))
fixefs = as.data.frame(fixef(Olmo_main_individual_constraints))[2:5,] %>%
  mutate(percent_greater_zero = c(
  sum(coeffs$Culture > 0) / length(coeffs$Culture),
  sum(coeffs$Power > 0) / length(coeffs$Power),
  sum(coeffs$Freq > 0) / length(coeffs$Freq),
  sum(coeffs$Len > 0) / length(coeffs$Len))) %>%
  mutate(percent_greater_zero = percent_greater_zero * 100)
```

## OLMo 2 1B

```{r}

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)



Olmo2_1b_main_genpref = brm(log_odds ~ GenPref,
                       data = data_validation_olmo2_1b,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo2_1b_model1_main'
                      )

fixef(Olmo2_1b_main_genpref)


Olmo2_1b_main_genpref_prompt2 = brm(log_odds ~ GenPref,
                       data = data_validation2_olmo2_1b,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo2_1b_main_genpref_prompt2'
                      )

fixef(Olmo2_1b_main_genpref_prompt2)


Olmo2_1b_main_genpref_prompt3 = brm(log_odds ~ GenPref,
                       data = data_validation3_olmo2_1b,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo2_1b_main_genpref_prompt3'
                      )

fixef(Olmo2_1b_main_genpref_prompt3)


Olmo2_1b_main_genpref_prompt4 = brm(log_odds ~ GenPref,
                       data = data_validation4_olmo2_1b,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/Olmo2_1b_main_genpref_prompt4'
                      )

fixef(Olmo2_1b_main_genpref_prompt4)


##### bigram probs##############################



data_validation_olmo2_1b = data_validation_olmo2_1b %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
       bigram2_alpha = paste0('and ', Word2),
       bigram1_nonalpha = paste0(Word2, ' and'),
       bigram2_nonalpha = paste0('and ', Word1)
         ) %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_validation2_olmo2_1b = data_validation2_olmo2_1b %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
       bigram2_alpha = paste0('and ', Word2),
       bigram1_nonalpha = paste0(Word2, ' and'),
       bigram2_nonalpha = paste0('and ', Word1)
         ) %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_validation3_olmo2_1b = data_validation3_olmo2_1b %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
       bigram2_alpha = paste0('and ', Word2),
       bigram1_nonalpha = paste0(Word2, ' and'),
       bigram2_nonalpha = paste0('and ', Word1)
         ) %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_validation4_olmo2_1b = data_validation4_olmo2_1b %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
       bigram2_alpha = paste0('and ', Word2),
       bigram1_nonalpha = paste0(Word2, ' and'),
       bigram2_nonalpha = paste0('and ', Word1)
         ) %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)




### models








########################################################






prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)
#lapse removed for having high VIF
#removed no_final_stress, intense, and percept because of backward model selection, lowest percentage of samples greater than/less than zero
#removing freq didn't affect the other variables
olmo2_1b_main_individual_constraints = brm(data = data_validation_olmo2_1b,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model3_main_olmo2_1b'
                      )

olmo2_1b_prompt2_individual_constraints = brm(data = data_validation2_olmo2_1b,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo2_1b_prompt2_individual_constraints'
                      )


olmo2_1b_prompt3_individual_constraints = brm(data = data_validation3_olmo2_1b,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo2_1b_prompt3_individual_constraints'
                      )


olmo2_1b_prompt4_individual_constraints = brm(data = data_validation4_olmo2_1b,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo2_1b_prompt4_individual_constraints'
                      )


percent_greater_zeros_prompt1_olmo2_1b = data.frame(fixef(olmo2_1b_main_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt1_olmo2_1b = data.frame(fixef(olmo2_1b_main_individual_constraints)) %>%
  mutate(Prefix = 'Next item: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt1_olmo2_1b = percent_greater_zeros_prompt1_olmo2_1b %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt1_olmo2_1b = fixefs_prompt1_olmo2_1b %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt1_olmo2_1b$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)



percent_greater_zeros_prompt2_olmo2_1b = data.frame(fixef(olmo2_1b_prompt2_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt2_olmo2_1b = data.frame(fixef(olmo2_1b_prompt2_individual_constraints)) %>%
  mutate(Prefix = 'example: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt2_olmo2_1b = percent_greater_zeros_prompt2_olmo2_1b %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt2_olmo2_1b = fixefs_prompt2_olmo2_1b %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt2_olmo2_1b$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

percent_greater_zeros_prompt3_olmo2_1b = data.frame(fixef(olmo2_1b_prompt3_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt3_olmo2_1b = data.frame(fixef(olmo2_1b_prompt3_individual_constraints)) %>%
  mutate(Prefix = 'instance: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt3_olmo2_1b = percent_greater_zeros_prompt3_olmo2_1b %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt3_olmo2_1b = fixefs_prompt3_olmo2_1b %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt3_olmo2_1b$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


percent_greater_zeros_prompt4_olmo2_1b = data.frame(fixef(olmo2_1b_prompt4_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt4_olmo2_1b = data.frame(fixef(olmo2_1b_prompt4_individual_constraints)) %>%
  mutate(Prefix = 'try this: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt4_olmo2_1b = percent_greater_zeros_prompt4_olmo2_1b %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt4_olmo2_1b = fixefs_prompt4_olmo2_1b %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt4_olmo2_1b$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


fixefs_prompt1_olmo2_1b = fixefs_prompt1_olmo2_1b %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt2_olmo2_1b = fixefs_prompt2_olmo2_1b %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt3_olmo2_1b = fixefs_prompt3_olmo2_1b %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt4_olmo2_1b = fixefs_prompt4_olmo2_1b %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))



fixefs_all_prompts_olmo2_1b = fixefs_prompt1_olmo2_1b %>%
  full_join(fixefs_prompt2_olmo2_1b) %>%
  full_join(fixefs_prompt3_olmo2_1b) %>%
  full_join(fixefs_prompt4_olmo2_1b) %>%
  select(Prefix, Coefficient, Estimate, Est.Error, `Q2.5`, `Q97.5`, `% Samples > 0`)

#write_csv(fixefs_all_prompts, '../Data/fixefs_all_prompts.csv')


fixefs_olmo2_1b = data.frame(fixef(olmo2_1b_main_individual_constraints, summary = F)) %>%
  select(-Intercept) %>%
  summarise(across(everything(), ~ {
    med = median(.x, na.rm = TRUE)
    if (med > 0) {
      mean(.x > 0, na.rm = TRUE) # Proportion of positive values
    } else if (med < 0) {
      mean(.x < 0, na.rm = TRUE) # Proportion of negative values
    } else {
      NA # Median is exactly zero
    }
  }))

fixefs_olmo2_1b$min_col=colnames(fixefs_olmo2_1b)[apply(fixefs_olmo2_1b, 1, which.min)]

fixefs_all_prompts_olmo2_1b
fixefs_olmo2_1b
```


## GPT 2 XL

```{r}

prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)



gpt2xl_main_genpref = brm(log_odds ~ GenPref,
                       data = data_validation_gpt2xl,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_model1_main'
                      )

fixef(gpt2xl_main_genpref)


gpt2xl_main_genpref_prompt2 = brm(log_odds ~ GenPref,
                       data = data_validation2_gpt2xl,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_main_genpref_prompt2'
                      )

fixef(gpt2xl_main_genpref_prompt2)


gpt2xl_main_genpref_prompt3 = brm(log_odds ~ GenPref,
                       data = data_validation3_gpt2xl,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_main_genpref_prompt3'
                      )

fixef(gpt2xl_main_genpref_prompt3)


gpt2xl_main_genpref_prompt4 = brm(log_odds ~ GenPref,
                       data = data_validation4_gpt2xl,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_main_genpref_prompt4'
                      )

fixef(gpt2xl_main_genpref_prompt4)






######################################################################### individ. constraints
prior_probs = c(
  prior(student_t(3, 0, 1), class = 'Intercept'),
  prior(student_t(3, 0, 1), class = 'sigma'),
  prior(student_t(3, 0, 1), class = 'b')
)
#lapse removed for having high VIF
#removed no_final_stress, intense, and percept because of backward model selection, lowest percentage of samples greater than/less than zero
#removing freq didn't affect the other variables
gpt2xl_main_individual_constraints = brm(data = data_validation_gpt2xl,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/model3_main_gpt2xl'
                      )

gpt2xl_prompt2_individual_constraints = brm(data = data_validation2_gpt2xl,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_prompt2_individual_constraints_'
                      )


gpt2xl_prompt3_individual_constraints = brm(data = data_validation3_gpt2xl,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_prompt3_individual_constraints'
                      )


gpt2xl_prompt4_individual_constraints = brm(data = data_validation4_gpt2xl,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_prompt4_individual_constraints'
                      )


percent_greater_zeros_prompt1_gpt2xl = data.frame(fixef(gpt2xl_main_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt1_gpt2xl = data.frame(fixef(gpt2xl_main_individual_constraints)) %>%
  mutate(Prefix = 'Next item: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt1_gpt2xl = percent_greater_zeros_prompt1_gpt2xl %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt1_gpt2xl = fixefs_prompt1_gpt2xl %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt1_gpt2xl$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)



percent_greater_zeros_prompt2_gpt2xl = data.frame(fixef(gpt2xl_prompt2_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt2_gpt2xl = data.frame(fixef(gpt2xl_prompt2_individual_constraints)) %>%
  mutate(Prefix = 'example: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt2_gpt2xl = percent_greater_zeros_prompt2_gpt2xl %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt2_gpt2xl = fixefs_prompt2_gpt2xl %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt2_olmo2_1b$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

percent_greater_zeros_prompt3_gpt2xl = data.frame(fixef(gpt2xl_prompt3_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt3_gpt2xl = data.frame(fixef(gpt2xl_prompt3_individual_constraints)) %>%
  mutate(Prefix = 'instance: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt3_gpt2xl = percent_greater_zeros_prompt3_gpt2xl %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt3_gpt2xl = fixefs_prompt3_gpt2xl %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt3_gpt2xl$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


percent_greater_zeros_prompt4_gpt2xl = data.frame(fixef(gpt2xl_prompt4_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

fixefs_prompt4_gpt2xl = data.frame(fixef(gpt2xl_prompt4_individual_constraints)) %>%
  mutate(Prefix = 'try this: ') %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)


percent_greater_zeros_prompt4_gpt2xl = percent_greater_zeros_prompt4_gpt2xl %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefs_prompt4_gpt2xl = fixefs_prompt4_gpt2xl %>%
  mutate(percent_greater_zero = percent_greater_zeros_prompt4_gpt2xl$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)


fixefs_prompt1_gpt2xl = fixefs_prompt1_gpt2xl %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt2_gpt2xl = fixefs_prompt2_gpt2xl %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt3_gpt2xl = fixefs_prompt3_gpt2xl %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))
fixefs_prompt4_gpt2xl = fixefs_prompt4_gpt2xl %>%
  mutate(Coefficient = c('Intercept', 'Culture', 'Power', 'Freq', 'Len'))



fixefs_all_prompts_gpt2xl = fixefs_prompt1_gpt2xl %>%
  full_join(fixefs_prompt2_gpt2xl) %>%
  full_join(fixefs_prompt3_gpt2xl) %>%
  full_join(fixefs_prompt4_gpt2xl) %>%
  select(Prefix, Coefficient, Estimate, Est.Error, `Q2.5`, `Q97.5`, `% Samples > 0`)

#write_csv(fixefs_all_prompts, '../Data/fixefs_all_prompts.csv')


fixefs_gpt2xl = data.frame(fixef(gpt2xl_main_individual_constraints, summary = F)) %>%
  select(-Intercept) %>%
  summarise(across(everything(), ~ {
    med = median(.x, na.rm = TRUE)
    if (med > 0) {
      mean(.x > 0, na.rm = TRUE) # Proportion of positive values
    } else if (med < 0) {
      mean(.x < 0, na.rm = TRUE) # Proportion of negative values
    } else {
      NA # Median is exactly zero
    }
  }))

fixefs_gpt2xl$min_col=colnames(fixefs_gpt2xl)[apply(fixefs_gpt2xl, 1, which.min)]

fixefs_gpt2xl
fixefs_all_prompts_gpt2xl
```

# Added a bunch of prefixes --> single model with prefix as RE?

```{r}
corpus = read_csv('../Data/nonce_and_attested_binoms.csv')

data_all_prefixes_all_models_attested_novel = read_csv('../Data/ALL_MODELS_ALL_PREFIXES_NOVE_AND_ATTESTED.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5)

data_all_prefixes_all_models = read_csv('../Data/ALL_MODELS_ALL_PREFIXES_NOVE_AND_ATTESTED.csv') %>%
  mutate(log_odds = preference) %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5) %>%
  filter(Attested == 0)





bigram_counts = read_csv('../Data/olmo_bigram_freqs.csv')
onegram_corpus_size = 1560674367427
count_and = 43378012800

bigram_counts = bigram_counts %>%
  mutate(ngram = str_replace_all(ngram, '\t', ' '))


data_all_prefixes_all_models = data_all_prefixes_all_models %>%
  rename(no_final_stress = `*BStress`)



data_all_prefixes_all_models = data_all_prefixes_all_models %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
         bigram2_alpha = paste0('and ', Word2),
         bigram1_nonalpha = paste0(Word2, ' and'),
         bigram2_nonalpha = paste0('and ', Word1)
         )

data_all_prefixes_all_models = data_all_prefixes_all_models %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_all_prefixes_all_models = data_all_prefixes_all_models %>%
  mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
         bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
         log_bigram_prob_alpha = log(bigram_prob_alpha),
         log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
         log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)




```

## Model

```{r}

options(contrasts = c("contr.sum","contr.sum"))

prior_probs = c(
  prior(student_t(3, 0, 5), class = 'Intercept'),
  prior(student_t(3, 0, 5), class = 'sigma'),
  prior(student_t(3, 0, 5), class = 'b')
)

data_all_prefixes_all_models$prompt_value = factor(data_all_prefixes_all_models$prompt_value)

all_models_all_prefixes = brm(log_odds ~ GenPref * model + (GenPref * model | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_all_models,
                       prior = prior_probs,
                       iter = 20000,
                       warmup = 10000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_all_prefixes'
                      )


data_all_prefixes_all_models_attested_novel_and_attested = data_all_prefixes_all_models_attested_novel_and_attested %>%
  mutate(RelFreq = case_when(
    Attested == 1 ~ RelFreq,
    Attested == 0 ~ 0),
    OverallFreq = case_when(
      Attested == 1 ~ OverallFreq,
      Attested == 0 ~ 0
  ))
data_all_prefixes_all_models_attested_novel$Attested = factor(data_all_prefixes_all_models_attested_novel_and_attested$Attested)



all_models_all_prefixes_bigrams = brm(log_odds ~ GenPref * model * log_bigram_odds_ratio + (GenPref * model | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_all_models,
                       prior = prior_probs,
                       iter = 20000,
                       warmup = 10000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_all_prefixes_bigrams'
                      )

fixef(all_models_all_prefixes)
fixef(all_models_all_prefixes_bigrams)


data_all_prefixes_gpt2xl = data_all_prefixes_all_models %>%
  filter(model == 'gpt2xl')

data_all_prefixes_olmo2_1b = data_all_prefixes_all_models %>%
  filter(model == 'olmo2_1b')

data_all_prefixes_olmo7b = data_all_prefixes_all_models %>%
  filter(model == 'olmo7b')

data_all_prefixes_gpt2 = data_all_prefixes_all_models %>%
  filter(model == 'gpt2')

data_all_prefixes_gptoss = data_all_prefixes_all_models %>%
  filter(model == 'gptoss120b')

all_models_gpt2xl = brm(log_odds ~ GenPref + (GenPref | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gpt2xl,
                       prior = prior_probs,
                       iter = 20000,
                       warmup = 10000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_gpt2xl_all_prefixes'
                      )

all_models_olmo2_1b = brm(log_odds ~ GenPref + (GenPref | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_olmo2_1b,
                       prior = prior_probs,
                       iter = 22000,
                       warmup = 11000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.99, max_treedepth = 17),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_olmo2_1b_all_prefixes'
                      )


prior_probs = c(
  prior(student_t(3, 0, 0.25), class = 'Intercept'),
  prior(student_t(3, 0, 0.25), class = 'sigma'),
  prior(student_t(3, 0, 0.25), class = 'b')
)

all_models_olmo7b = brm(log_odds ~ GenPref + (GenPref | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_olmo7b,
                       prior = prior_probs,
                       iter = 30000,
                       warmup = 15000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.999, max_treedepth = 20),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_olmo7b_all_prefixes'
                      )

all_models_gpt2 = brm(log_odds ~ GenPref + (GenPref | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gpt2,
                       prior = prior_probs,
                       iter = 30000,
                       warmup = 15000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.999, max_treedepth = 20),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_gpt2'
                      )

all_models_gptoss = brm(log_odds ~ GenPref + (GenPref | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gptoss,
                       prior = prior_probs,
                       iter = 30000,
                       warmup = 15000,
                       chains = 4,
                       cores = 4,
                       control = list(adapt_delta=0.999, max_treedepth = 20),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_gptoss'
                      )

fixef(all_models_all_prefixes)
fixef(all_models_gpt2xl)
fixef(all_models_olmo2_1b)
fixef(all_models_olmo7b)
fixef(all_models_gpt2)
fixef(all_models_gptoss)

```

### random effects plots
```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# 1. reshape wide (all terms and all metrics)
re_df = ranef(all_models_all_prefixes)$prompt_value

re_df <- as.data.frame.table(re_df, responseName = "value")
names(re_df) <- c("prompt_value", "metric", "term", "value")


re_wide <- re_df %>%
  pivot_wider(
    names_from = c(term, metric),
    values_from = value
  ) %>%
  mutate(
    model3_Estimate = -(model1_Estimate + model2_Estimate),
    model3_Q2.5     = -(model1_Q97.5   + model2_Q97.5),
    model3_Q97.5    = -(model1_Q2.5    + model2_Q2.5)
  )

# 2. pivot long again so ggplot can facet
re_long <- re_wide %>%
  pivot_longer(
    cols = -prompt_value,
    names_to = c("term","metric"),
    names_sep = "_",
    values_to = "value"
  )

# 3. only keep useful terms
plot_df <- re_long %>%
  filter(term %in% c("Intercept","GenPref","model1","model2","model3"))

# 4. create wide data for ribbons
plot_df_wide <- plot_df %>%
  pivot_wider(names_from = metric, values_from = value)

# 5. plot EVERYTHING properly
ggplot(plot_df_wide, aes(x = Estimate, y = prompt_value)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), height = 0) +
  facet_wrap(~ term, scales = "fixed") +
  theme_bw()


```

```{r}
# 1. reshape wide (all terms and all metrics)
re_df = ranef(all_models_all_prefixes_bigrams)$prompt_value

re_df <- as.data.frame.table(re_df, responseName = "value")
names(re_df) <- c("prompt_value", "metric", "term", "value")


re_wide <- re_df %>%
  pivot_wider(
    names_from = c(term, metric),
    values_from = value
  ) %>%
  mutate(
    model3_Estimate = -(model1_Estimate + model2_Estimate),
    model3_Q2.5     = -(model1_Q97.5   + model2_Q97.5),
    model3_Q97.5    = -(model1_Q2.5    + model2_Q2.5)
  )

# 2. pivot long again so ggplot can facet
re_long <- re_wide %>%
  pivot_longer(
    cols = -prompt_value,
    names_to = c("term","metric"),
    names_sep = "_",
    values_to = "value"
  )

# 3. only keep useful terms
plot_df <- re_long %>%
  filter(term %in% c("Intercept","GenPref","model1","model2","model3"))

# 4. create wide data for ribbons
plot_df_wide <- plot_df %>%
  pivot_wider(names_from = metric, values_from = value)

# 5. plot EVERYTHING properly
ggplot(plot_df_wide, aes(x = Estimate, y = prompt_value)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), height = 0) +
  facet_wrap(~ term, scales = "fixed") +
  theme_bw()


```

### individual constraints

```{r}

all_models_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) * model + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_all_models,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/all_models_all_prefixes_individual_constraints'
                      )

gpt2xl_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gpt2xl,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2xl_all_prefixes_individual_constraints'
                      )

olmo2_1b_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_olmo2_1b,
                       prior = prior_probs,
                       iter = 6000,
                       warmup = 3000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo2_1b_all_prefixes_individual_constraints'
                      )

olmo7b_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_olmo7b,
                       prior = prior_probs,
                       iter = 8000,
                       warmup = 4000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/olmo7b_all_prefixes_individual_constraints'
                      )

# olmo7b_all_prefixes_individual_constraints_test = brm(log_odds ~ (Culture + Power + Freq + Len) + (1 | prompt_value) + ( 1 | Alpha),
#                        data = data_all_prefixes_olmo7b,
#                        prior = prior_probs,
#                        iter = 8000,
#                        warmup = 4000,
#                        chains = 4,
#                        cores = 4,
#                        #control = list(adapt_delta=0.99, max_treedepth = 15),
#                        #control = list(max_treedepth = 20),
#                        #file = '../Data/olmo7b_all_prefixes_individual_constraints'
#                       )

gpt2_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gpt2,
                       prior = prior_probs,
                       iter = 8000,
                       warmup = 4000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gpt2_all_prefixes_individual_constraints_test'
                      )


gptoss_all_prefixes_individual_constraints = brm(log_odds ~ (Culture + Power + Freq + Len) + (Culture + Power + Freq + Len | prompt_value) + ( 1 | Alpha),
                       data = data_all_prefixes_gptoss,
                       prior = prior_probs,
                       iter = 8000,
                       warmup = 4000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = '../Data/gptoss_all_prefixes_individual_constraints'
                      )

fixef(all_models_all_prefixes_individual_constraints)
fixef(gpt2xl_all_prefixes_individual_constraints)
fixef(olmo2_1b_all_prefixes_individual_constraints)
fixef(olmo7b_all_prefixes_individual_constraints)
fixef(gpt2_all_prefixes_individual_constraints)
fixef(gptoss_all_prefixes_individual_constraints)

```
```{r}
# 1. reshape wide (all terms and all metrics)
re_df = ranef(gpt2_all_prefixes_individual_constraints)$prompt_value

re_df <- as.data.frame.table(re_df, responseName = "value")
names(re_df) <- c("prompt_value", "metric", "term", "value")


re_wide <- re_df %>%
  pivot_wider(
    names_from = c(term, metric),
    values_from = value)
  # ) %>%
  # mutate(
  #   model3_Estimate = -(model1_Estimate + model2_Estimate),
  #   model3_Q2.5     = -(model1_Q97.5   + model2_Q97.5),
  #   model3_Q97.5    = -(model1_Q2.5    + model2_Q2.5)
  # )

# 2. pivot long again so ggplot can facet
re_long <- re_wide %>%
  pivot_longer(
    cols = -prompt_value,
    names_to = c("term","metric"),
    names_sep = "_",
    values_to = "value"
  )

# 3. only keep useful terms
plot_df <- re_long %>%
  filter(term %in% c("Intercept","Power","Freq","Len"))

# 4. create wide data for ribbons
plot_df_wide <- plot_df %>%
  pivot_wider(names_from = metric, values_from = value)

# 5. plot EVERYTHING properly
ggplot(plot_df_wide, aes(x = Estimate, y = prompt_value)) +
  geom_point() +
  geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), height = 0) +
  facet_wrap(~ term, scales = "fixed") +
  theme_bw()

```


# Plots

```{r}
fixefs_main_model = as.data.frame(fixef(Olmo_main_genpref)) %>%
  mutate(checkpoint = 'main')
  
fixefs_main_model$Parameter = rownames(fixefs_main_model)
rownames(fixefs_main_model) = NULL


models_all = fixefs_m1 %>%
  full_join(fixefs_main_model) %>%
  left_join(checkpoint_tokens_key)

models_all$checkpoint = factor(models_all$checkpoint, levels = c('step0-tokens0B', 'step500-tokens2B', 'step1000-tokens4B', 'step1500-tokens6B', 'step2000-tokens8B', 'step2500-tokens10B', 'step3000-tokens12B', 'step3500-tokens14B', 'step4000-tokens16B', 'step5000-tokens20B', 'step5500-tokens23B', 'step6000-tokens25B', 'step6500-tokens27B', 'step7000-tokens29B', 'step7500-tokens31B', 'step8000-tokens33B', 'step8500-tokens35B', 'step9000-tokens37B', 'step9500-tokens39B', 'step10000-tokens41B', 'step20000-tokens83B', 'step30000-tokens125B', 'step40000-tokens167B', 'step50000-tokens209B', 'step100000-tokens419B', 'step200000-tokens838B', 'step400000-tokens1677B', 'main'))


models_for_plotting = models_all %>%
  filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  mutate(checkpoint_numeric = as.numeric(factor(checkpoint))) 



plot_all_m1 = ggplot(data = models_for_plotting, aes(x=checkpoint_numeric, y = Estimate)) +
  geom_point() +
  geom_smooth(method='lm') +
  geom_errorbar(aes(ymin=Q2.5, ymax = Q97.5), position=position_dodge(0.05)) +
  scale_x_continuous(breaks = unique(models_for_plotting$checkpoint_numeric), labels = models_for_plotting$num_tokens) +
  theme_bw() #+
  #theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
  

plot_all_m1
```





```{r}
library(ggh4x)
data_main_model2 = data_main_model %>%
  mutate(checkpoint = 'main') %>%
  mutate(n_billion_tokens = 2050)

models_for_plotting2 = combined_df %>%
  full_join(data_main_model2) %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value') %>%
  filter(n_billion_tokens %in% c(0, 10, 20, 35, 125, 419, 1677, 2050))



models_for_plotting2$n_billion_tokens = factor(models_for_plotting2$n_billion_tokens, levels = c(0, 10, 20, 35, 125, 419, 1677, 2050),
                           labels = c('0 Tokens', '10 Billion Tokens', '20 Billion Tokens', '35 Billion Tokens', '125 Billion Tokens', '419 Billion Tokens', '1677 Billion Tokens', '2050 Billion Tokens'))

models_for_plotting2$constraint = factor(models_for_plotting2$constraint, levels = c("Culture", "Power", "Freq", "Len"), labels = c("Culture", "Power", "Freq", "Len"))

main_model_plot2 = ggplot(data = models_for_plotting2, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  facet_nested(constraint~n_billion_tokens) +
  #ggtitle('Number of Billions of Tokens') +
  theme_bw() +
  theme(axis.title.x = element_text(size = 10, face = 'bold'),
        axis.title.y = element_text(size = 10, face = 'bold')) +
  xlab('Constraint Value') +
  ylab('Log Odds') +
  scale_x_continuous(limits = c(-3, 3))

main_model_plot2
```



```{r}
main_model = data_main_model %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value')

main_model_plot = ggplot(data = main_model, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  facet_wrap(~constraint) +
  theme_bw()

main_model_plot
```

```{r}
gen_pref_data = data_main_model

main_model_plot = ggplot(data = gen_pref_data, aes(x=GenPref, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  #facet_wrap(~constraint) +
  theme_bw()

main_model_plot
```
