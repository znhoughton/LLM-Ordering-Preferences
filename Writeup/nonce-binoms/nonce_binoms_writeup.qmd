---
title: "Emergent Abstract Ordering Preferences in Large Language Models"
csl: apa.csl
format:
  elsevier-pdf:
    keep-tex: true  
    journal:
      name: None
      formatting: preprint #review #
      model: 1p
      layout: onecolumn
      cite-style: authoryear
  pdf:
    tbl-cap-location: bottom
    latex-engine: lualatex 
    number-sections: true
    number-depth: 3
    #font:
      #text: "Times New Roman"     # Default font for body text
      #heading: "Times New Roman"  # Default font for headings
    documentclass: article       # LaTeX document class
    mainfont: "Crimson" # Main font
    CJKmainfont: "Noto Serif KR"  # Font for Korean text
    keep-tex: true                # Save the intermediate .tex file for debugging
    link-citations: true          # Enable hyperlinked citations
    colorlinks: false             # Disable colored hyperlinks
    classoption: nottoc           # Prevent the Table of Contents from appearing in the TOC
    geometry:                     # Page geometry settings
      - left=1in
      - right=1in
      - marginparwidth=1.5in
      - twoside=true
editor: visual
author:
  - name: Zachary Nicholas Houghton
    email: znhoughton@ucdavis.edu
    affiliations:
      - name: University of California, Davis
  - name: Kenji Sagae
    affiliations:
      - name: University of California, Davis
  - name: Emily Morgan
    affiliations:
      - name: University of California, Davis
      
bibliography: ["r-references.bib"]

#header-includes:
#- \usepackage[section]{placeins}


abstract: |
---

```{r, include = F}
library(tidyverse)
library(brms)
library(here)
library(ggh4x)

data_main_model = read_csv(paste0(here("Data"), '/allenai_OLMo-7B-0424-hf.csv'))
corpus = read_csv(paste0(here("Data"), '/nonce_binoms.csv'))

data_main_model = data_main_model %>%
  mutate(ProbAandB = exp(`Alpha Probs`) / (exp(`Alpha Probs`) + exp(`Nonalpha Probs`))) %>%
  mutate(log_odds = `Alpha Probs` - `Nonalpha Probs`)


data_main_model = data_main_model %>%
  separate(binom, c('Word1', 'and', 'Word2'), remove = F, sep = ' ') %>%
  select(-and) %>%
  #mutate(across(2:3, tolower)) %>%
  left_join(corpus) %>%
  mutate(checkpoint = 'main') %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals)))
  #mutate(log_freq = log(OverallFreq))# %>%
  #mutate(OverallFreq = log_freq - mean(log_freq))# %>%
  #mutate(GenPref = GenPref - 0.5) %>%
  #mutate(RelFreq = RelFreq - 0.5)

data_main_model = data_main_model %>%
  rename(no_final_stress = `*BStress`)



data_main_model = data_main_model %>%
  mutate(bigram1_alpha = paste0(Word1, ' and'),
         bigram2_alpha = paste0('and ', Word2),
         bigram1_nonalpha = paste0(Word2, ' and'),
         bigram2_nonalpha = paste0('and ', Word1)
         )

bigram_counts = read_csv(paste0(here("Data"), '/olmo_bigram_freqs.csv'))
onegram_corpus_size = 1560674367427
count_and = 43378012800

bigram_counts = bigram_counts %>%
  mutate(ngram = str_replace_all(ngram, '\t', ' '))

data_main_model = data_main_model %>%
  left_join(bigram_counts, by = c('bigram1_alpha' = 'ngram')) %>%
  rename(count_bigram1_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_alpha' = 'ngram')) %>%
  rename(count_bigram2_alpha = count) %>%
  left_join(bigram_counts, by = c('bigram1_nonalpha' = 'ngram')) %>%
  rename(count_bigram1_nonalpha = count) %>%
  left_join(bigram_counts, by = c('bigram2_nonalpha' = 'ngram')) %>%
  rename(count_bigram2_nonalpha = count) %>%
  mutate(corpus_size = onegram_corpus_size) %>%
  mutate(count_and = count_and)


data_main_model = data_main_model %>%
  mutate(bigram_prob_alpha = (Word1_freq / corpus_size) * (count_bigram1_alpha / Word1_freq) * (count_bigram2_alpha / count_and),
         bigram_prob_nonalpha = (Word2_freq / corpus_size) * (count_bigram1_nonalpha / Word2_freq) * (count_bigram2_nonalpha / count_and),
         log_bigram_prob_alpha = log(bigram_prob_alpha),
         log_bigram_prob_nonalpha = log(bigram_prob_nonalpha),
         log_bigram_odds_ratio = log_bigram_prob_alpha - log_bigram_prob_nonalpha)

Olmo_main_genpref = brm(log_odds ~ GenPref,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here('Data'), '/model1_main')
                      )


Olmo_main_genpref_log_odds = brm(log_odds ~ GenPref * log_bigram_odds_ratio,
                       data = data_main_model,
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here('Data'), '/model2_main')
                      )



Olmo_main_individual_constraints = brm(data = data_main_model,
                       log_odds ~ Culture + Power + Freq + Len, #Icon and Form removed for only having one value
                       prior = prior_probs,
                       iter = 10000,
                       warmup = 5000,
                       chains = 4,
                       cores = 4,
                       #control = list(adapt_delta=0.99, max_treedepth = 15),
                       #control = list(max_treedepth = 20),
                       file = paste0(here('Data'), '/model3_main')
                      )

data_path = here("Data")



combined_df = read_csv(paste0(here("Data"), "/combined_df.csv"))

checkpoint_tokens_key = combined_df %>%
  group_by(checkpoint) %>%
  slice_head(n=1) %>%
  select(checkpoint, num_tokens, n_billion_tokens)


fixefs_m1 = read_csv(paste0(here("Data"), "/fixefs_m1.csv"))
fixefs_m3 = read_csv(paste0(here("Data"), "/fixefs_m3.csv"))
```

# Introduction {#sec-introduction}

Large language models have stormed the media in the last few years and become a popular topic in the scientific literature. Their historic rise to fame has brought with them many heated debates regarding whether large language models constitute human-like models of language or whether what they are doing is completely different from humans [@piantadosiChapterModernLanguage; @piantadosiMeaningReferenceLarge2022; @benderDangersStochasticParrots2021].

Many of these debates have centered around the tradeoff between computation and storage: how much are these models simply reproducing from their training data vs how much of their productions are novel utterances using learned linguistic patterns. On one hand, there is no doubt that large language models store and reproduce large chunks of language. In fact, OpenAI is even being sued by *The New York Times* for allegedly reproducing entire articles verbatim [@nyt_v_openai]. This sentiment – that large language models are nothing but glorified copy cats – has been echoed by several other prominent linguists [@bender2020climbing; @benderDangersStochasticParrots2021; c.f., @piantadosiChapterModernLanguage].

Specifically, proponents of the "LLMs as copy cats" argument have pointed out that large language models are trained on an inconceivably large amount of data. For example, the OLMo models were trained on trillions of tokens [@groeneveldOLMoAcceleratingScience2024][^1]. As such, it is extremely difficult to determine whether utterances produced by an LLM are truly novel, or whether they are simply reproduced from their training data. This is further complicated by the fact that training data for LLMs is typically either not publicly available, or so huge that it's incredibly difficult to work with. On the other hand, it is clear that large language models are learning at least some linguistic patterns. For example, @mccoy2023much demonstrated that Chat GPT-2 is able to generate well-formed novel words as well as well-formed novel syntactic structures, however they found that it still copies extensively.

[^1]: This is magnitudes larger than the 350 million words that the average college-aged speaker has seen in their lifetime [@levy2012].

<!--# Need to double check the stochastic parrots paper -->

A similar debate in the field has centered around whether large language models learn any knowledge about the meaning of words. For example, @bender2020climbing have argued that large language models, which are only trained on the form, have no way of learning anything about meaning. They pointed out that large language models do not have the rich information that humans receive, such as the referent of the form. However, @piantadosiMeaningReferenceLarge2022 have countered by arguing that co-occurrence statistics can be extremely informative about a word's meaning. For example, they argued that many words, such as "justice", contain no clear referent and instead have to be learned by humans based on the context that they occur in. It seems plausible that large language models could learn at least some information about meanings similarly.

<!--# debate about meaning bender octopus vs piantadosi -->

These debates, however, have been highly theoretical and speculative and very few empirical studies have been done to actually investigate these questions [c.f., @lebrun2022evaluating; @mccoy2023much; @lasri2022subject]. Thus in the present paper we address these debates by taking an in-depth look at large language models' abilities to learn generalized word order patterns that are contingent on word meanings.

Our specific contributions are as follows: We make a 1-gram, 2-grams, and 3-grams corpus of Dolma [@soldainiDolmaOpenCorpus2024] along with the scripts to reproduce it open-access. We also use this corpus to create novel binomials (Noun *and* Noun compounds, such as *cats and dogs*) that the OLMo 7B model [@groeneveldOLMoAcceleratingScience2024] has never seen. We examine whether OLMo 7B learns general patterns for ordering preferences (e.g., preferring the binomial ordering with the shorter word first as in *ladies in gentlemen* as opposed to gentlemen *and ladies*) for binomials that it has never seen before. Finally, we demonstrate a timescale of these preferences emerging over training that can be used to generate predictions about human learning.

## Abstractions in Large Language Models

The evidence for learned abstractions in large language models is extremely mixed. For example, @haley2020bert demonstrated that many of the BERT models are not able to reliably determine the plurality of novel words. Additionally, @liAreNeuralNetworks2021 demonstrated that when tasked with producing the correct tense for a word, BERT tends to rely on memorization from its training data as opposed to learning the more general linguistic pattern.

On the other hand, @lasri2022subject demonstrated that BERT can generalize well to novel subject-verb pairs. Specifically, they tested BERT's performance on novel sentences along with semantically incoherent but syntactically sensible sentences (e.g., *colorless green ideas sleep furiously*). They found that BERT performs well on items it wasn't trained on. Additionally, @liAssessingCapacityTransformer2023 demonstrated that transformers are able to use abstract knowledge to correctly predict subject-verb and object-past participle agreements in French. Similarly, as mentioned earlier, @mccoy2023much examined to what extent GPT-2 was simply copying its training data vs producing novel utterances. They found that while GPT-2 copies extensively, it also produces both novel words as well as novel syntactic structures.

There is also evidence that transformer models can learn abstractions from other domains as well. For example, @tartaglini2023deep examined the ability of a transformer model in a same-different task (i.e., determining if two entities, e.g., two shapes, in an image are the same or different). They found that certain models can reach near perfect accuracy on items they have never seen before. They argued that this demonstrates their abilities to learn abstract representations.

Finally, there's evidence that inducing abstractions facilitates performance in large language models. For example @zhengTakeStepBack2024 used a novel prompting technique to enable LLMs to use abstractions when reasoning. They found that LLMs hallucinate less when they implement abstractions in their reasoning. Similarly, @mccoyUniversalLinguisticInductive2020 demonstrated that large language models can use abstractions to learn language more easily, suggesting that inducing abstractions may help reduce the amount of training that large language models require.

## Abstractions in Humans

Abstractions have been a part of just about every linguistic theory out there, including both generativist and non-generativist theories. This is for good reason, too: one of the hallmarks of human language learning is the ability to produce novel, never-heard-before utterances. In order to do so, most theories posit that humans leverage their remarkable ability to learn linguistic patterns beyond simple co-occurrence rates [c.f., @ambridge2020]. For example, when presented a novel noun, children are able to consistently produce the proper plural form of that noun [@berko]. Similarly, children are able to abstract across different contexts to learn a word's general meaning [@yu2007rapid].

Abstractions are useful because when humans produce a novel utterance that they have never heard before, their novel utterances contain a level of systematicity that allows the interlocutor to understand it with very little difficulty. This is even the case for binomials (e.g., *cat and dog*), whose order does not particularly affect the meaning of the utterance.

Binomial ordering preferences are well-documented in the literature. Binomials have been a particularly useful test case in the psycholinguistics literature because humans have varying preferences for which noun occurs first in the binomial, despite the ordering having little effect on the meaning. For example, while one could reasonably say *computers and monitors* or *monitors and computers*, *butter and bread* sounds quite unnatural compared to *bread and butter*. However, none of those have a particularly different meaning.

There have been several studies showing human ordering preferences for binomials are driven, at least in part, by abstract ordering preferences [@morganAbstractKnowledgeDirect2016; @morganFrequencydependentRegularizationIterated2016a; @morganModelingIdiosyncraticPreferences2015; @morganProductiveKnowledgeItemspecific2024] For example, @morganAbstractKnowledgeDirect2016 demonstrated that humans show ordering preferences for binomials beyond simply preferring the more frequent ordering. In order to demonstrate this, they coded a list of binomials for a variety of semantic constraints, phonological constraints, and metric constraints that affect human ordering preferences for binomials [@benorChickenEggProbabilistic2006]. They found that human ordering preferences were driven by abstract ordering preferences, such as a preference to place short words before longer words, even after accounting for the relative frequency (the proportion of each ordering that a binomial occurs in corpus data). However, humans are also sensitive to the frequency of each ordering (e.g., a preference for *bread and butter* over *butter and bread*). In other words, human ordering preferences are driven by both the observed preferences in corpus data (i.e., the number of times they've encountered each ordering of the binomial) as well as by abstract ordering preferences [@morganAbstractKnowledgeDirect2016]. Interestingly, more recently @morganProductiveKnowledgeItemspecific2024 also demonstrated abstract ordering preferences exert a constant effect throughout the frequency spectrum. That is, abstract ordering preferences have an effect on human ordering preferences even for high-frequency binomials.

In summary, human ordering preferences for a given binomial cannot be predicted purely from the proportion of occurrences in the alphabetical order (e.g., *bread and butter*) to the occurrences in nonalphabetical (e.g., *butter and bread*). This suggests that humans are not simply reproducing their input, but learning abstract ordering preferences from the data.

## Present Study

In the present study we examine whether large language models are simply copying their input, or whether they are behaving more similarly to humans and learning abstract linguistics patterns. We use binomials as a test case because human ordering preferences deviate from the observed preferences for them. Further, we use novel binomials that OLMo has never seen before. Therefore any preference the model has cannot be driven by experience with the specific item.

In Experiment 1 we examine whether OLMo's 7B model [@groeneveldOLMoAcceleratingScience2024] is sensitive to abstract ordering preferences for novel binomials that the model has never seen before. We also examine the individual constraints that drive abstract ordering preferences in humans, such as the preference for short words before long words, to determine whether OLMo is sensitive to the same constraints in the same way as humans. In Experiment 2, we examine the same questions at different stages of the model's training in order to determine how these abstract ordering preferences emerge as a function of the training.

# Dataset

## Dolma

For both experiments, we use the dataset described in this section. In order to examine whether large language models learn preferences above and beyond simply memorizing co-occurrence rates, we created a 1-grams, 2-grams, and 3-grams corpus of Dolma [@soldainiDolmaOpenCorpus2024]. Specifically, we used Dolma version 1_7 (2.05 trillion tokens), which was used to train OLMo-7B-v1.7 [@groeneveldOLMoAcceleratingScience2024]. Our corpus contains every n-gram (ignoring punctuation and capitalization) in the Dolma corpus, as well as the number of times that n-gram appeared.

We then created a list of binomials and searched the corpus to find a list of binomials that did not occur in the Dolma corpus. We eliminated binomials which occurred more than zero times in either their alphabetical or nonalphabetical orderings. Thus, OLMo has had no experience with either ordering of any of our binomials. We also used the 2-grams corpus to get their bigram frequencies and the 1-grams corpus to get their individual word frequencies.[^2] Our full list of items comprises 131 binomials.

[^2]: Our code and datasets for the analyses run in this paper can also be found at: [github.com/\[anonymous\]](github.com/%5Banonymous%5D).

<!--# revise footnote with actual github link. This should probably get its own github repo, so I will need to change this as well. -->

## Abstract Ordering Preferences Corpus

In order to examine whether large language models are learning preferences similar to humans, we calculated the abstract ordering preference value for each of our binomials [following @morganAbstractKnowledgeDirect2016]. @morganAbstractKnowledgeDirect2016 demonstrated that their model's estimated abstract ordering preference value is a significant predictor of human binomial ordering preferences, even after accounting for the frequency of each ordering. Abstract ordering preferences are calculated from a mix of semantic and phonological properties that human binomial ordering preferences have been shown to be sensitive to [@benorChickenEggProbabilistic2006]. For each of these constraints, a positive value indicates a preference for the alphabeticaly first word to be placed first (a neutral reference order). A negative value indicates a preference for the nonalphabetical word to be placed first. For example, a positive value of *Freq* indicates that the alphabetical word is more frequent and thus is predicted to be placed first, while a negative value indicates that the nonalphabetical word is more frequent. The constraints are as follows [taken from @morganModelingIdiosyncraticPreferences2015]:

-   **Length**: The shorter word should appear first, e.g. *abused and neglected*.

-   **No Final Stress**: The final syllable of the second word should not be stressed, e.g. *abused and neglected*.

-   **Lapse:** Avoid unstressed syllables in a row, e.g. *FARMS and HAY-fields* vs *HAY-fields and FARMS*

-   **Frequency**: The more frequent word comes first, e.g. *bride and groom*.

-   **Formal Markedness**: The word with more general meaning or broader distribution comes first, e.g. *boards and two-by-fours*.

-   **Perceptual Markedness**: Elements that are more closely connected to the speaker come first. This constraint encompasses @cooper1975world's (1975) \`Me First' constraint and includes numerous subconstraints, e.g.: animates precede inanimates; concrete words precede abstract words; e.g. *deer and trees*.

-   **Power**: The more powerful or culturally prioritized word comes first, e.g. *clergymen and parishioners*.

-   **Iconic/scalar sequencing**: Elements that exist in sequence should be ordered in sequence, e.g. *achieved and maintained*.

-   **Cultural Centrality**: The more culturally central or common element should come first, e.g. *oranges and grapefruits*.

-   **Intensity**: The element with more intensity appear first, e.g. *war and peace*.

# Experiment 1

In Experiment 1, we examine whether OLMo-7B's ordering preferences are driven by abstract ordering preferences for novel binomials. In order to do so, we created a list of binomials and searched the Dolma corpus we created to confirm that they did not occur in either alphabetical or nonalphabetical ordering. Both authors then coded the binomials for each of the constraints mentioned earlier and disagreements were resolved by discussion. We then examined whether OLMo-7B shows any preference for one ordering over the other for each binomial. If OLMo has developed any abstract ordering preferences, it should show a systematic preference for one ordering over the other. If it is just reproducing the binomials in ordering based purely off the frequency of the items in its input, we should see only an effect of the frequency constraint (i.e., it should simply show a preference to place the more frequent word first).

## Methods

### Language Model Predictions

For each model, we calculated the ordering preferences of the alphabetical form (a neutral reference order) for each of our 131 binomials in the dataset. The predicted probability of the alphabetical form was calculated as the product of the model's predicted probability of each word in the binomial. In order to accurately calculate the probability of the first word in the binomial, each binomial was given the prefix "Next item: ". Thus the probability of the alphabetical form, *A and B,* is:

$$
\begin{aligned}
    P_{alphabetical} & = P(A|\text{`Next item:'} )\\      
    & \times P(and|\text{`Next item: A'})\\      
    & \times P(B|\text{`Next item: A and'})
\end{aligned}
$$ {#eq-probalpha}

\noindent where *A* is the alphabetically first word in the binomial and *B* is the other word. Similarly, the probability of the nonalphabetical form, *B and A*, is:

$$
\begin{aligned}    
P_{nonalphabetical} & = P(B|\text{`Next item:'})\\
& \times P(and|\text{`Next item: B'})\\
& \times P(A|\text{`Next item: B and'})
\end{aligned}
$$ {#eq-probnonalpha}

Finally, we calculated the log odds ratio of the probability of the alphabetical form to the probability of the nonalphabetical form to obtain a single numeric value representing the overall ordering preference for a given binomial. A larger positive value represents a preference for the alphabetical form and a larger negative value represents a preference for the nonalphabetical form:

$$
LogOdds(AandB) = log(\frac{P_{alphabetical}}{P_{nonalphabetical}})
$$

### Analyses

We present two mixed-effects analyses using Bayesian linear regression models, implemented in *brms* [@brms] with weak, uninformative priors. For each of our models, the intercept represents the grand mean and the coefficient estimates represent the distance from the grand mean. Bayesian statistics don't force us into a binary interpretation of significance or non-significance, however we can consider an estimate to be statistically significant if the credible interval for that estimate excludes zero.

For both analyses, the dependent variable is LogOdds(AandB), which was described above. Our dependent variable in the first analysis is the abstract ordering preference for each binomial (AbsPref)[^3]. Our dependent variables in the second analysis are the individual constraints that are used to calculate AbsPref. The model equations are below in @eq-m1 and @eq-m2. Note that Formal Markedness and Iconicity were dropped from the second model because the constraint values were zero for all of the binomials. Further, our constraints demonstrated a level of co-linearity. Co-linearity can result in poor model estimates and inflated credible intervals. In order to deal with this, we dropped the constraint with the highest variance inflation factor (which turned out to be the lapse constraint). We then performed backward model selection and dropped the predictors whose credible intervals were most centered around zero. This resulted in dropping the no final stress, intense, and percept constraints. We acknowledge that this approach is quite exploratory and thus interpretations at the level of the individual constraint must be taken with a grain of salt.

[^3]: We also ran a model examining whether the ordering preferences were driven by the bigram probabilities of the binomial, however surprisingly bigram probabilities turned out to be a poor predictor of ordering preferences.

$$
LogOdds(AandB) \sim AbsPref
$$ {#eq-m1}

$$
LogOdds(AandB) \sim Culture + Power + Freq + Len
$$ {#eq-m2}

## Results

The results for the first analysis are presented below in @tbl-exp1m1. Our results suggest that there is a main-effect of abstract ordering preference for OLMo's 7B model. A visualization of these results can be found below in @fig-exp1m1.

```{r, echo = F, message = F}
#| label: tbl-exp1m1
#| tbl-cap: "Model results examining the effect of AbsPref on LogOdds(AandB)."

#table 1
fixefsm1 = as.data.frame(fixef(Olmo_main_genpref)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)

rownames(fixefsm1) = c('Intercept', 'AbsPref')

knitr::kable(fixefsm1, booktabs = T)
```

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-exp1m1
#| fig-cap: "Visualization of the effects of AbsPref on LogOdds(AandB)"

gen_pref_data = data_main_model 



main_model_plot = ggplot(data = gen_pref_data, aes(x=GenPref, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  ylab('LogOdds(AandB)') +
  xlab('Abstract Ordering Preference') +
  #facet_wrap(~constraint) +
  theme_bw()

main_model_plot

```

While these results suggest that the large language models' ordering preferences are sensitive to similar factors as humans, it's unclear whether this similarity holds on the level of the individual constraints. Thus, in the second analysis we examine which specific constraints the model is sensitive to, and to what extent.[^4] For this analysis, following @houghtonTaskdependentConsequencesDisfluency2024, we also present the percentage of posterior samples greater than zero. The results of this analysis can be found below in @tbl-exp1m2.

[^4]: It's also helpful to consider how many of our binomials the constraint even applied to (i.e., how many binomials were the constraints non-zero). For the Culture constraint, 62 of our 131 binomials had a non-zero value. For the Power constraint, 54 did, for the Frequency constraint, all 131 binomials did, and for the Len constraint, 85 did.

```{r, echo = F, message = F}
#| label: tbl-exp1m2
#| tbl-cap: "Model results examining the effect of each individual constraint on LogOdds(AandB)."

percent_greater_zero = data.frame(fixef(Olmo_main_individual_constraints, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

#table2
fixefsm2 = as.data.frame(fixef(Olmo_main_individual_constraints)) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3)





percent_greater_zero = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'Culture', 'Power', 'Freq', 'Len')))


fixefsm2 = fixefsm2 %>%
  mutate(percent_greater_zero = percent_greater_zero$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  rename('% Samples > 0' = `percent_greater_zero`)

rownames(fixefsm2) = c('Intercept', 'Culture', 'Power', 'Freq', 'Len')

knitr::kable(fixefsm2, booktabs = T)
```

The model is most sensitive to the Power constraint, however there appears to be a marginal effect of Culture as well, since nearly 95% of the posterior samples are greater than zero despite the credible interval crossing zero. Surprisingly, there also appears to be a negative effect of length with a slight preference to place the longer word first, which is the opposite direction from what we see in humans. Length is often correlated with frequency, since frequent words tend to be shorter. As such, we ran a model without frequency to determine whether the negative effect of length was do to co-linearity with frequency. However, dropping frequency from the model did not affect the effect of length. Further, we also ran a model with only length as the predictor and for that model as well the estimate of length remained negative.

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F, eval = F}
#| label: fig-exp1m2
#| fig-cap: "Visualization of the effects of each individual constraint on LogOdds(AandB)."
#removed this figure since Emily isn't a fan
main_model = data_main_model %>%
  rename('Final Stress' = no_final_stress) %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value')


main_model_plot = ggplot(data = main_model, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  xlab('Constraint Value') +
  ylab('LogOdds(AandB)') +
  facet_wrap(~constraint) +
  theme_bw()

main_model_plot

```

## Discussion

The present experiment found that OLMo-7B has learned abstract ordering preferences even for novel binomials that it has never seen before. Further, these ordering preferences aren't simply based on the individual word frequencies. Specifically, we find a main-effect of abstract ordering preferences on the model's binomial ordering preferences. Additionally, we find a strong preference to place the more powerful word first, a weak preference to place the more culturally central word first, and a weak preference to place the longer word first.

These results together suggest that the model is learning abstract ordering preferences but these are not identical to humans. For example, while humans also show a preference for placing the more powerful and more culturally central words first, humans also prefer to place the *shorter* word first [@morganAbstractKnowledgeDirect2016; @morganModelingIdiosyncraticPreferences2015]. However, we find the opposite finding: large language models prefer to place the longer word first. One explanation for this is a difference in terms of the input between humans and large language models. The length constraint is determined by the number of syllables. Syllables are salient cues in the audio that humans receive during learning \[**NEED CITATION\]**, but it's less clear how salient of a cue this is for large language models, which receive sub-word tokens (which vary in their size, from being individual orthographic symbols to being entire words[^5]).

[^5]: For example, both *ictionary* and *region* are individual tokens in OpenAI's models (<https://gist.github.com/s-macke/ae83f6afb89794350f8d9a1ad8a09193>).

# Experiment 2

In Experiment 1 we demonstrated that large language models are not simply copying their training, but are learning some abstract ordering preferences from their input. However, OLMo makes public various checkpoints during the model's training, thus allowing us the opportunity to examine how these preferences arise as a function of the training. Thus, in Experiment 2 we examine the evolution of these learned abstract ordering preferences as the model learns over time.

## Methods

### Language Model Predictions

Our language model predictions in Experiment 2 were obtained using the same procedure as in Experiment 1. However, instead of calculating these metrics only for the main model, we calculated them at various checkpoints. These checkpoints are listed below, in terms of the steps as well as the number of billions of tokens the model had been trained on at that checkpoint:

-   Step 0, 0B Tokens

-   Step 1000, 2B Tokens

-   Step 10000, 41B Tokens

-   Step 50000, 209B Tokens

-   Step 100000, 419B Tokens

-   Step 200000, 838B Tokens

-   Step 400000, Tokens 1677B

### Analysis

We ran the same two analyses as in Experiment 1, however, we ran these analyses for the each of the checkpoints listed above.

## Results

Our model estimates for the effect of AbsPref on LogOdds(AandB) at each checkpoint are presented below in @tbl-exp2m1 and visualized in @fig-exp2m1.

```{r, echo = F, message = F}
#| label: tbl-exp2m1
#| tbl-cap: "Model results examining the effect of AbsPref on LogOdds(AandB) for each checkpoint."

#table 1
fixefsexp2m1 = fixefs_m1 %>%
  left_join(checkpoint_tokens_key) %>%
  filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  arrange(n_billion_tokens) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  select(-n_billion_tokens, -checkpoint, -Parameter) 


fixefsexp2m1 = fixefsexp2m1[,c('num_tokens', 'Estimate', 'Est.Error', 'Q2.5', 'Q97.5')]
fixefsexp2m1 = rename(fixefsexp2m1, 'Number of Tokens' = num_tokens)

knitr::kable(fixefsexp2m1, booktabs = T)

```

The model results are visualized below in @fig-exp2m1.

```{r, echo = F, out.width = '70%', fig.align = 'center', warning = F, message = F}
#| label: fig-exp2m1
#| fig-cap: "Visualization of the model predictions for the effect of AbsPref on LogOdds(AandB) for each checkpoint."


fixefs_main_model = as.data.frame(fixef(Olmo_main_genpref)) %>%
  mutate(checkpoint = 'main')
  
fixefs_main_model$Parameter = rownames(fixefs_main_model)
rownames(fixefs_main_model) = NULL


models_all = fixefs_m1 %>%
  full_join(fixefs_main_model) %>%
  left_join(checkpoint_tokens_key)

models_all$checkpoint = factor(models_all$checkpoint, levels = c('step0-tokens0B', 'step500-tokens2B', 'step1000-tokens4B', 'step1500-tokens6B', 'step2000-tokens8B', 'step2500-tokens10B', 'step3000-tokens12B', 'step3500-tokens14B', 'step4000-tokens16B', 'step5000-tokens20B', 'step5500-tokens23B', 'step6000-tokens25B', 'step6500-tokens27B', 'step7000-tokens29B', 'step7500-tokens31B', 'step8000-tokens33B', 'step8500-tokens35B', 'step9000-tokens37B', 'step9500-tokens39B', 'step10000-tokens41B', 'step20000-tokens83B', 'step30000-tokens125B', 'step40000-tokens167B', 'step50000-tokens209B', 'step100000-tokens419B', 'step200000-tokens838B', 'step400000-tokens1677B', 'main'))


models_for_plotting = models_all %>%
  filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  mutate(checkpoint_numeric = as.numeric(factor(checkpoint))) 



plot_all_m1 = ggplot(data = models_for_plotting, aes(x=checkpoint_numeric, y = Estimate)) +
  geom_point() +
  geom_smooth(method='lm') +
  geom_errorbar(aes(ymin=Q2.5, ymax = Q97.5), position=position_dodge(0.05)) +
  scale_x_continuous(breaks = unique(models_for_plotting$checkpoint_numeric), labels = models_for_plotting$num_tokens) +
  ylab('Coefficient Estimate') +
  xlab('Number of Tokens') +
  theme_bw() #+
  #theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
  

plot_all_m1
```

Our results demonstrate that it takes quite a large number of tokens for the model to learn the abstract ordering preferences. As @fig-exp2m1 demonstrates, the effect of abstract ordering preference isn't convincing until the model has experienced 419 billion tokens. However, it does appear that the model develops a slight preference quite rapidly. For example, by 2 billion tokens there appears to be a very slight (though unconvincing) effect of abstract ordering preferences on the ordering of binomials.

Similar to Experiment 1, in our second analysis we present a breakdown of the effects of each individual constraint. In this analysis, however, we demonstrate the effect of each constraint at each checkpoint. The full table results can be found in the @sec-individual-constraints-at-each-checkpoint, but we present a visualization below in @fig-exp2m2.

```{r echo = F, fig.width=8, fig.height=9.5, fig.align = 'center', warning = F, message = F}
#| label: fig-exp2m2
#| fig-cap: "Visualization of the effect of each constraint on the ordering preference at each checkpoint."

data_main_model2 = data_main_model %>%
  mutate(checkpoint = 'main') %>%
  mutate(n_billion_tokens = 2050)

models_for_plotting2 = combined_df %>%
  full_join(data_main_model2) %>%
  pivot_longer(c(Culture, Power, Freq, Len), names_to = 'constraint', values_to = 'constraint_value') %>%
  filter(n_billion_tokens %in% c(0, 10, 20, 35, 125, 419, 1677, 2050))



models_for_plotting2$n_billion_tokens = factor(models_for_plotting2$n_billion_tokens, levels = c(0, 10, 20, 35, 125, 419, 1677, 2050),
                           labels = c('0', '10B', '20B', '35B', '125B', '419B', '1677B', '2050B'))

models_for_plotting2$constraint = factor(models_for_plotting2$constraint, levels = c("Culture", "Power", "Freq", "Len"), labels = c("Culture", "Power", "Freq", "Len"))

main_model_plot2 = ggplot(data = models_for_plotting2, aes(x=constraint_value, y = log_odds)) +
  geom_point(alpha=0.5, color = 'darkblue') +
  geom_smooth(method = 'lm', formula = y ~ x, se = TRUE, linewidth = 1) +
  facet_nested(constraint~n_billion_tokens) +
  #ggtitle('Number of Billions of Tokens') +
  theme_bw() +
  theme(axis.title.x = element_text(size = 15, face = 'bold'),
        axis.title.y = element_text(size = 15, face = 'bold'),
        strip.text.x = element_text(size = 12, face = 'bold'),
        strip.text.y = element_text(size = 12, face = 'bold'),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 12)) +
  xlab('Constraint Value') +
  ylab('Log Odds') +
  scale_x_continuous(limits = c(-3, 3))

main_model_plot2

```

Interestingly, it appears that early on the model already shows evidence of learning human-like preferences. For example, by 10 billion tokens, the model has learned to place more powerful words first and shorter words first. However, the model seems slower to learn to place more culturally central words first. Further, as it receives more training the effect of length undergoes a reversal in direction.

## Discussion

Our results demonstrate that OLMo learns human-like ordering preferences early on for most of the constraints, but takes longer to learn human-like ordering preferences for the culture constraint. Further, the model is human-like in its predictions for length early on, but as it receives more training data it learns the opposite length prediction. It is unclear what exactly is causing this reversal, but as we suggested earlier it may be a function of the tokenization differences between human input and large language models' input. We look forward to examining this question in more depth in future studies.

Our results can also be interpreted as predictions for human data. For example, it takes the model longer to learn the Culture constraint than the other constraints. Is the same true for humans?

Finally, previous results suggest that for non-novel binomials that OLMo has experienced before, it relies almost exclusively on its experience with it in its training data (i.e., its ordering preference is primarily driven by the proportion of occurrences in alphabetical to nonalphabetical ordering) and does not use abstract ordering preferences at all [@HoughtonMorgan2025]. Our results thus suggest that while large language models are able to learn abstract ordering preferences, in cases where they've seen the binomial they are able to rely on their experience more than a human can.

# Conclusion

In the present study, we examined the ordering preferences in OLMo 7B's main model as well as the model at various stages in learning. We found that the main model shows human-like ordering preferences, with the exception of a preference for longer words before shorter words. Further, we show that while the effect of abstract ordering preference on a whole takes a great deal of time (over 400 billion tokens to be convincing), the model seems to pick up on individual constraints quite early on, and initially even learns the correct direction of the length constraint.

Our results suggest that large language models are not simply copying their input, but are learning interesting, human-like phenomena from their training. However, they are not learning identically to humans, as demonstrated by the opposite direction of the length preference. This is not surprising given the differences in tokenization methods. Further, while humans rely on abstract preferences even for binomials that they have encountered before, large language models only rely on abstract ordering preferences for items that they have not encountered at all.

# Limitations

The main limitation is the number of models tested. We only tested one model in this study, so it's possible that other large language models may demonstrate different ordering preferences. However, we believe that the advantages of demonstrating an in-depth analysis of a single model outweigh a more broad analysis of several models, especially given the lack of easily available open access training data, which is crucial to guaranteeing that the model has not encountered our items before.

Additionally, we only test one construction in this paper (binomials). While it is possible that abstract ordering preferences for binomials are different than other constructions, binomials are well understood in the human linguistics literature thus making them a good test case for our analyses.

\clearpage

# References {.unnumbered}

::: {#refs}
:::

\newpage

# Appendices {.unnumbered}

\appendix

\renewcommand{\thesection}{\Alph{section}}

\setcounter{section}{0}

\counterwithin{figure}{section}

\counterwithin{table}{section}

# Full List of Stimuli {#sec-full-list-of-stimuli .appendix}

Below is a table of our list of binomials as well as the individual constraint values for each.

```{r echo = F, message = F}
#| label: tbl-stimuli
#| tbl-cap: "Full list of binomials as well as their constraints."

stimuli_list = corpus %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  select(Word1, Word2, Form, Percept, Culture, Power, Intense, Icon, Freq, Len, Lapse, `*BStress`, GenPref) %>%
  mutate_if(is.numeric,
            round,
            digits=3) %>%
  rename('AbsPref' = GenPref)

stimuli_list = rename(stimuli_list, 'Final Stress' = `*BStress`)

knitr::kable(stimuli_list, booktabs = T)
```

\clearpage

# Individual Constraints at Each Checkpoint {#sec-individual-constraints-at-each-checkpoint .appendix}

Below is a table of the fixed-effects for each individual constraint at each checkpoint.

```{r, echo = F, message = F}
#| label: tbl-exp2m2
#| tbl-cap: "Model results examining the effect of each individual constraint on LogOdds(AandB)."

#table 1
fixefsexp2m3 = fixefs_m3 %>%
  left_join(checkpoint_tokens_key) %>%
  #filter(Parameter == 'GenPref') %>%
  filter(n_billion_tokens %in% c(0, 2, 41, 209, 419, 838, 1677)) %>%
  arrange(n_billion_tokens) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f',
            digits = 3) %>%
  select(-n_billion_tokens, -checkpoint) 


fixefsexp2m3 = fixefsexp2m3[,c('Parameter', 'num_tokens', 'Estimate', 'Est.Error', 'Q2.5', 'Q97.5')]

knitr::kable(fixefsexp2m3, booktabs = T)
```
